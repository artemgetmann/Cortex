<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Reinforcement Learning for Self-Improving Agent with Skill Library</title>
<!--Generated on Thu Dec 18 21:50:15 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2512.17102v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S1" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S2" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S3" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S3.SS1" title="In 3 Method ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Skill Library Agent</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S3.SS2" title="In 3 Method ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>SAGE for Skill Library Agent</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S3.SS2.SSS1" title="In 3.2 SAGE for Skill Library Agent ‚Ä£ 3 Method ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Preliminary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S3.SS2.SSS2" title="In 3.2 SAGE for Skill Library Agent ‚Ä£ 3 Method ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Sequential Rollout</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S3.SS2.SSS3" title="In 3.2 SAGE for Skill Library Agent ‚Ä£ 3 Method ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Skill-integrated Reward</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S3.SS2.SSS4" title="In 3.2 SAGE for Skill Library Agent ‚Ä£ 3 Method ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>SAGE</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.SS1" title="In 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.SS2" title="In 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.SS3" title="In 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Skill Library Usage Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.SS4" title="In 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S5" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A1" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Interaction Format Example of Skill Library Agent</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A2" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>SAGE with Longer Task Chain in Sequential Rollout</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A3" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>AppWorld Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A4" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Prompt for Skill Library Agent</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A5" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Training Details of Baseline GRPO</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A6" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Expert Experience Dataset Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A7" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Training Details of SAGE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A8" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Task Execution Examples with Different Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A9" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>Retrieval Method Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A9.SS1" title="In Appendix I Retrieval Method Ablation Study ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I.1 </span>Details of Retrieval Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A9.SS2" title="In Appendix I Retrieval Method Ablation Study ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I.2 </span>Further Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A10" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">J </span>Reward Design Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A10.SS1" title="In Appendix J Reward Design Ablation Study ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">J.1 </span>Details of Reward Designs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A10.SS2" title="In Appendix J Reward Design Ablation Study ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">J.2 </span>Skill Library Usage Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A11" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">K </span>RL Initialization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12" title="In Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">L </span>SFT Initialized Baseline GRPO</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Reinforcement Learning for Self-Improving Agent with Skill Library</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiongxiao Wang<sup class="ltx_sup" id="id10.10.id1"><span class="ltx_text ltx_font_italic" id="id10.10.id1.1">1</span></sup> ‚ÄÉQiaojing Yan<sup class="ltx_sup" id="id11.11.id2"><span class="ltx_text ltx_font_italic" id="id11.11.id2.1">2</span></sup> ‚ÄÉYawei Wang<sup class="ltx_sup" id="id12.12.id3"><span class="ltx_text ltx_font_italic" id="id12.12.id3.1">2</span></sup> ‚ÄÉYijun Tian<sup class="ltx_sup" id="id13.13.id4"><span class="ltx_text ltx_font_italic" id="id13.13.id4.1">2</span></sup> ‚ÄÉSoumya Smruti Mishra<sup class="ltx_sup" id="id14.14.id5"><span class="ltx_text ltx_font_italic" id="id14.14.id5.1">2</span></sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.6.1">Zhichao Xu<sup class="ltx_sup" id="id6.6.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.1.1.1">2</span></sup></span> ‚ÄÉ<span class="ltx_text ltx_font_bold" id="id7.7.2">Megha Gandhi<sup class="ltx_sup" id="id7.7.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.2.1.1">2</span></sup></span> ‚ÄÉ<span class="ltx_text ltx_font_bold" id="id8.8.3">Panpan Xu<sup class="ltx_sup" id="id8.8.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id8.8.3.1.1">2</span></sup></span> ‚ÄÉ<span class="ltx_text ltx_font_bold" id="id9.9.4">Lin Lee Cheong<sup class="ltx_sup" id="id9.9.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id9.9.4.1.1">2</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id15.15.id6">1</sup>University of Wisconsin‚ÄìMadison; <sup class="ltx_sup" id="id16.16.id7">2</sup>AWS Agentic AI 
<br class="ltx_break"/>
<span class="ltx_text ltx_font_typewriter" id="id17.17.id8">jwang2929@wisc.edu; {qiaojiny, yawenwan, yijunt, soumish, xzhichao, ganmegha, xupanpan, lcheong}@amazon.com</span>
</span><span class="ltx_author_notes">Work was done during an internship at AWS Agentic AI.Corresponding author: Panpan Xu, Email: xupanpan@
<br class="ltx_break"/>amazon.com</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id18.id1">Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging.
To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents‚Äô self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework‚Äôs key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards.
Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.9">
<p class="ltx_p ltx_align_center" id="p1.9.10"><span class="ltx_text ltx_font_bold" id="p1.9.10.1">Reinforcement Learning for Self-Improving Agent with Skill Library</span></p>
<p class="ltx_p ltx_align_center" id="p1.9.9" style="width:345.0pt;"><span class="ltx_text ltx_inline-block" id="p1.9.9.9" style="width:0.0pt;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_top" id="p1.9.9.9.9">
<span class="ltx_thead">
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.5.5.5.5.5.5"><span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.5.5">Jiongxiao Wang<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.1.1">1</span></sup><span class="ltx_note ltx_role_thanks" id="p1.5.5.5.5.5.5.5.2"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">thanks: </span>Work was done during an internship at AWS Agentic AI.</span></span></span> ‚ÄÉ‚ÄäQiaojing Yan<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.3.1">2</span></sup> ‚ÄÉ‚ÄäYawei Wang<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.4.1">2</span></sup> ‚ÄÉ‚ÄäYijun Tian<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.5"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.5.1">2</span></sup> ‚ÄÉ‚ÄäSoumya Smruti Mishra<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.6"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.6.1">2</span></sup></span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.9.9.9.9.9">
<span class="ltx_td ltx_align_right" id="p1.9.9.9.9.9.4"><span class="ltx_text ltx_font_bold" id="p1.6.6.6.6.6.1.1">Zhichao Xu<sup class="ltx_sup" id="p1.6.6.6.6.6.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.6.6.6.6.6.1.1.1.1">2</span></sup></span> ‚ÄÉ<span class="ltx_text ltx_font_bold" id="p1.7.7.7.7.7.2.2">Megha Gandhi<sup class="ltx_sup" id="p1.7.7.7.7.7.2.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.7.7.7.2.2.1.1">2</span></sup></span> ‚ÄÉ<span class="ltx_text ltx_font_bold" id="p1.8.8.8.8.8.3.3">Panpan Xu<sup class="ltx_sup" id="p1.8.8.8.8.8.3.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.8.8.8.8.8.3.3.1.1">2</span></sup><span class="ltx_note ltx_role_thanks" id="p1.8.8.8.8.8.3.3.2"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">thanks: </span><span class="ltx_text ltx_font_medium" id="p1.8.8.8.8.8.3.3.2.1">Corresponding author: Panpan Xu, Email: xupanpan@ ‚ÄÇ‚Ää
amazon.com</span></span></span></span></span> ‚ÄÉ<span class="ltx_text ltx_font_bold" id="p1.9.9.9.9.9.4.4">Lin Lee Cheong<sup class="ltx_sup" id="p1.9.9.9.9.9.4.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.9.9.9.9.9.4.4.1.1">2</span></sup></span></span></span>
<span class="ltx_tr" id="p1.9.9.9.9.10.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.9.9.9.9.10.1.1"><sup class="ltx_sup" id="p1.9.9.9.9.10.1.1.1">1</sup>University of Wisconsin‚ÄìMadison; <sup class="ltx_sup" id="p1.9.9.9.9.10.1.1.2">2</sup>AWS Agentic AI</span></span>
<span class="ltx_tr" id="p1.9.9.9.9.11.2">
<span class="ltx_td ltx_align_center" id="p1.9.9.9.9.11.2.1">
<span class="ltx_text ltx_font_typewriter" id="p1.9.9.9.9.11.2.1.1">jwang2929@wisc.edu; {qiaojiny, yawenwan, yijunt, soumish, xzhichao, ganmegha, xupanpan, lcheong}@amazon.com</span></span></span>
</span>
</span></span></p>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language model (LLM)-based agents have been widely applied to automate complex tasks through active environmental interactions, including coding agent <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2024swe</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">novikov2025alphaevolve</span>)</cite>, deep research <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">deep_research</span>)</cite>, assistant agent <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yao2024tau</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span></cite>,
and web browsing <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">yao2022webshop</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhouwebarena</span>)</cite>. To enhance the performance of these multi-turn interactive agents, researchers have successfully integrated reinforcement learning (RL) techniques into their frameworks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">qiwebrl</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhou2025sweet</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025ragen</span>)</cite>. Recent advances, particularly in reinforcement learning with verifiable rewards (RLVR) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">shao2024deepseekmath</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">guo2025deepseek</span>)</cite>, have enabled effective end-to-end agent training for improved performance <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">jin2025search</span>)</cite>. However, despite RL‚Äôs effectiveness, significant limitations persist: RL-trained agents are often limited to specific training scenarios <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zheng2024gpt</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024effects</span>)</cite>. When deployed in new environments, they struggle to demonstrate continual learning capabilities to effectively utilize valuable on-going experiences for future tasks.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address these limitations, one potential solution is enabling agents to transform their previous interaction experiences into reusable skills, which can be stored in a skill library for future reference. When agents encounter similar tasks, these previously acquired skills can be leveraged through experience replay to improve task success rates, particularly in scenarios that were not encountered during training but experienced during deployment. Furthermore, since each skill is composed of a list of actions, utilization of these skills can enhance agent efficiency by condensing complex action sequences into reusable operations.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="228" id="S1.F1.g1" src="intro.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration for Skill Library Agent and Sequential Rollout with Skill-integrated Reward.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recent research has made significant strides in enabling agents to compose reusable skills <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wangvoyager</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">cailarge</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">nguyen2024dynasaur</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2024agent</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zheng2025skillweaver</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025inducing</span>)</cite>. For instance, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025inducing</span></cite> focuses on inducing high-level web browsing skills from successful action trajectories, transforming primitive actions like <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">click</span> and <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">search</span> into more complex operations such as <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">search product</span>. While these works have demonstrated the effectiveness of skill libraries for deployed agents, they predominantly rely on manually crafted prompts for skill generation and utilization. This prompt-based approach, however, is inherently constrained by the instruction following capabilities of the base model, limiting both the quality and adaptability of the skill library.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we explore enhancing agents‚Äô self-improvement capabilities through RL with a skill library. Given the diverse frameworks of agents for various tasks, we focus specifically on tool-using agents that interact with environments through API calls and receive corresponding feedback. Our implementation extends the CodeAct framework <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2024executable</span>)</cite>, which enables agents to compose code by combining multiple APIs with basic programming constructs (such as for loops) to solve complex tasks. Building upon this foundation, we develop a specialized framework for self-improving agents with skill library (referred to as <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">skill library agents</span> for simplicity). Unlike previous frameworks such as Agent Skill Induction <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025inducing</span>)</cite>, which typically define reusable skills only after task completion, we implement a unified format for both task solving and skill generation, following <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nguyen2024dynasaur</span></cite>. In our approach, when the agent model interacts with the environment through APIs, it generates programmatic functions that can be saved as skills and subsequently called to execute, rather than using multiple APIs directly. Given that in-context prompting struggles to adapt open-source models to this newly proposed skill library agent, supervised fine-tuning (SFT) is first applied before RL using high-quality trajectories collected through expert experience generated by advanced LLMs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Based on the SFT model, we propose a novel RL framework for skill library agents. Traditional RL approaches typically consider rewards only for individual examples, limiting their scope to ongoing task performance.
For skill library agents, the focus is on developing high-quality, reusable skills while improving accurate skill usage from the library for task optimization.
To realize this, we extend Group Relative Policy Optimization (GRPO) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">shao2024deepseekmath</span>)</cite> and propose Skill Augmented GRPO for self-Evolution (SAGE), consisting of <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Sequential Rollout</span> and <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">Skill-integrated Reward</span>. Instead of single-task trajectories, the agent is trained with chains of similar tasks. We implement the Sequential Rollout process on the task chain, where skills generated in the previous tasks are preserved and made available for use in the following ones. Under this framework, the final Skill-integrated Reward is computed as the sum of two components: the verifiable outcome-based reward and an extra reward for high-quality skill generation and utilization.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To evaluate the effectiveness of SAGE for skill library agents, we conducted experiments on the AppWorld dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">trivedi2024appworld</span>)</cite>, where agents interact with their environment through API documentation lookup, API calls, and logical programming constructs to solve complex practical tasks. AppWorld utilizes the Scenario Goal Completion (SGC) metric, which measures the success rate of scenarios where all three similar tasks within a scenario are successfully completed. This metric effectively evaluates how well generated skills transfer across similar tasks. After applying SAGE to Qwen2.5-32B-Instruct, we observe significant improvements compared to prompting-based approaches. Our method achieves more than 3x SGC score while requiring less than half of generated tokens. Analysis of skill utilization reveals that agents trained with SAGE demonstrate more than 2x success rates when utilizing learned skills.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Furthermore, our approach achieves state-of-the-art performance on both Test Normal and Test Challenge datasets compared to previous training methods which also applied RL but without a skill library. For example, our method achieves 72.0% Task Goal Completion (TGC) and 60.7% SGC with an average of 12.1 interaction steps and 1,475 generated tokens on the Test Normal set. This represents substantial improvement over baseline training with GRPO, which achieved 69.2% TGC and 51.8% SGC while requiring 16.4 average steps and 3,613 tokens. These improvements demonstrate the effectiveness of our approach in enhancing both task performance and efficiency through the skill library agent.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">LLM-based Agent.</span> Recent advancements in instruction-tuned LLMs have enabled them to follow user instructions for interacting with external environments as autonomous agents. Various frameworks have been developed to enhance these backbone LLMs‚Äô capabilities in performing agentic tasks. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yao2023react</span></cite> pioneered a "reason-then-act" pipeline, guiding LLMs to generate interactive actions for agents after a text reasoning process. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">erdoganplan</span></cite> extended this approach by incorporating an additional planning phase. Additionally, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2024executable</span></cite> demonstrated that generating executable Python code and then run it within a code interpreter could significantly improve agent performance. To further enhance agent capabilities, researchers have applied both supervised fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">schick2023toolformer</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2023fireact</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zeng2024agenttuning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2024agentohana</span>)</cite> and reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">song2024trial</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">bai2024digirl</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025ragen</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">qiwebrl</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhou2025sweet</span>)</cite> to develop more effective and adaptive agent backbone LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Self-Improving Agent with Skill Library.</span> While RL algorithms have enabled agents to self-improve through valuable experiences explored during rollouts <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhouproposer</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">putta2024agent</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">qiwebrl</span>)</cite>, enabling continuous self-improvement after deployment, especially in new environments, remains challenging. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">wangvoyager</span></cite> pioneered the use of a skill library to record successful behaviors for later retrieval in Minecraft exploration. Subsequently, numerous studies have demonstrated the effectiveness of such skill libraries across various agentic tasks, including web exploration <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2024agent</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zheng2025skillweaver</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025inducing</span>)</cite>, computer control <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhengsynapse</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">wu2024copilot</span>)</cite>, and math problems <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">nguyen2024dynasaur</span>)</cite>. These skills can take two forms: natural language experience memories serving as a reference, or executable skills that can be directly implemented in the environment. In this paper, we focus on leveraging RL to enhance agents‚Äô self-improvement capabilities by teaching them to generate executable skills for the skill library during test time.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first detail the skill library agent. We then present our new RL framework specifically designed to integrate the skill library during training process. An illustrative figure of our skill library agent and Sequential Rollout with Skill-integrated Reward design is shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Skill Library Agent</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Before implementing RL, we first need to develop a self-improving agent that integrates the skill library for tool-using agents. This framework will serve as the foundation for RL rollout process and task evaluations.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Existing frameworks for skill library agents, such as Agent Skill Induction <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2025inducing</span>)</cite>, Agent Workflow Memory <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2024agent</span>)</cite>, and Voyager <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wangvoyager</span>)</cite>, typically define reusable skills after completing entire tasks. While this approach allows agents to observe complete task trajectories before determining skill definitions, it limits the RL process in two ways: (1) In long-horizon tasks, the additional skill generation process further extends the context length, potentially exceeding the model‚Äôs limitations; (2) The separation between task execution and skill generation creates an inconsistency that may impact learning effectiveness.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">To address these limitations, we follow the DynaSaur approach <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nguyen2024dynasaur</span></cite> and implement a unified format for both task solving and skill generation. Specifically, when the agent model interacts with the API environments, it first generates a skill function and then calls it to process, instead of using multiple APIs. An example of the format difference is presented in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A1" title="Appendix A Interaction Format Example of Skill Library Agent ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.9">Formally, given a task set <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1" intent=":literal"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>, our agent is designed to perform online learning from start to finish with a skill library <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics></math>, which can be initialized with either an empty set or previously defined skills. For each task <math alttext="q\sim Q" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3" intent=":literal"><semantics><mrow><mi>q</mi><mo>‚àº</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">q\sim Q</annotation></semantics></math>, the agent first retrieves a skill subset <math alttext="[a_{1},...,a_{k}]" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m4" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[a_{1},...,a_{k}]</annotation></semantics></math> from the skill library <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS1.p4.5.m5" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics></math> and adds them into context before performing the task. After that, the agent can automatically perform the following actions to interact with the skill library: (1) <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.9.1">Skill Usage</span>: Perform skill <math alttext="a_{i}\in[a_{1},...,a_{k}]" class="ltx_Math" display="inline" id="S3.SS1.p4.6.m6" intent=":literal"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>‚àà</mo><mrow><mo stretchy="false">[</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">a_{i}\in[a_{1},...,a_{k}]</annotation></semantics></math> to process the task; (2) <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.9.2">Skill Generation</span>: Define a skill function <math alttext="\hat{a}" class="ltx_Math" display="inline" id="S3.SS1.p4.7.m7" intent=":literal"><semantics><mover accent="true"><mi>a</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{a}</annotation></semantics></math> composed of multiple actions aimed at solving the task and then immediately call it to process the task; (3) <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.9.3">Skill Update</span>: If the skill <math alttext="a" class="ltx_Math" display="inline" id="S3.SS1.p4.8.m8" intent=":literal"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>, either from the skill library or newly defined, fails to execute, update the skill and recall it to process the task; (4) <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.9.4">Skill Save</span>: If the skill can be executed without error, add the new skill or update the existing skill in the skill library <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS1.p4.9.m9" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics></math>. Additionally, direct API calls are allowed for cases where defining a function skill is unnecessary for task processing.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>SAGE for Skill Library Agent</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We begin the description of our SAGE framework by introducing the preliminary GRPO algorithms, followed by our specifically-designed components for skill library agents: Sequential Rollout and Skill-integrated Reward.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Preliminary</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.3">In this paper, we build our SAGE based on GRPO <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">shao2024deepseekmath</span>)</cite>. For each query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1" intent=":literal"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math>, GRPO first samples a group of outputs <math alttext="\{o_{1},...,o_{G}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>o</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msub><mi>o</mi><mi>G</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{o_{1},...,o_{G}\}</annotation></semantics></math> from the old policy <math alttext="\pi_{\theta_{old}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3" intent=":literal"><semantics><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>d</mi></mrow></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{old}}</annotation></semantics></math> and then optimizes the policy by maximizing the following objective:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A12.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{J}_{\text{GRPO}}(\theta)=" class="ltx_Math" display="inline" id="S3.Ex1.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ùí•</mi><mtext>GRPO</mtext></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>Œ∏</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{J}_{\text{GRPO}}(\theta)=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{[q\sim Q,\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)]}" class="ltx_Math" display="inline" id="S3.Ex1.m2" intent=":literal"><semantics><msub><mi>ùîº</mi><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>q</mi><mo>‚àº</mo><mi>Q</mi></mrow><mo>,</mo><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mo>‚àº</mo><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>d</mi></mrow></msub></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><mi>O</mi><mo fence="false">|</mo><mi>q</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></msub><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{[q\sim Q,\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)]}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\{\min[s_{i,t}\hat{A}_{i,t}," class="ltx_math_unparsed" display="inline" id="S3.Ex2.m1" intent=":literal"><semantics><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>G</mi></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover></mstyle><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></munderover></mstyle><mrow><mo stretchy="false">{</mo><mi>min</mi><mrow><mo stretchy="false">[</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>,</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\{\min[s_{i,t}\hat{A}_{i,t},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\text{clip}\left(s_{i,t},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}]-\beta\mathbb{D}_{\text{KL}}\}" class="ltx_math_unparsed" display="inline" id="S3.Ex3.m1" intent=":literal"><semantics><mrow><mrow><mtext>clip</mtext><mrow><mo>(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>œµ</mi><mo>)</mo></mrow><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><mo>‚àí</mo><mi>Œ≤</mi><msub><mi>ùîª</mi><mtext>KL</mtext></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\displaystyle\text{clip}\left(s_{i,t},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}]-\beta\mathbb{D}_{\text{KL}}\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p1.11">where <math alttext="s_{i,t}=\frac{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,&lt;t})}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.4.m1" intent=":literal"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo fence="false">|</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>d</mi></mrow></msub></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo fence="false">|</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">s_{i,t}=\frac{\pi_{\theta}(o_{i,t}|q,o_{i,&lt;t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,&lt;t})}</annotation></semantics></math>, <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.5.m2" intent=":literal"><semantics><mi>œµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> is the clip ratio and <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.6.m3" intent=":literal"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> is the ratio of KL penalty between policy model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.7.m4" intent=":literal"><semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> and reference model <math alttext="\pi_{ref}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.8.m5" intent=":literal"><semantics><msub><mi>œÄ</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{ref}</annotation></semantics></math>. <math alttext="\hat{A}_{i,t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.9.m6" intent=":literal"><semantics><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\hat{A}_{i,t}</annotation></semantics></math> is the advantage calculated based on the relative rewards of the outputs inside each group. Given rewards <math alttext="\mathbf{r}=\{r_{1},r_{2},...,r_{G}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.10.m7" intent=":literal"><semantics><mrow><mi>ùê´</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{r}=\{r_{1},r_{2},...,r_{G}\}</annotation></semantics></math> of the outputs under the same group, the advantage is defined as <math alttext="\hat{A}_{i,t}=\frac{r_{i}-\text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.11.m8" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mrow><mtext>mean</mtext><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê´</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mrow><mtext>std</mtext><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê´</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\hat{A}_{i,t}=\frac{r_{i}-\text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Sequential Rollout</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">A principled method to augment agents‚Äô self-improving ability with skill library is through end-to-end RL. However, skill generation and usage processes often need multiple tasks to reveal the quality of skills. One potential solution to enabling the end-to-end RL is that instead of one task, we could give the agent a chain of tasks. Then a sequential rollout process is performed across these tasks, enabling the agent to progressively accumulate skills in its library throughout the task chain. Any skills learned in earlier tasks could be used in subsequent tasks. In this way, rewards signal from the successful usage of skills in later tasks can be back-propagated to the skill generation in the previous tasks. To ensure that the generated skills can be immediately applied to subsequent tasks, we construct the sequential tasks with examples under the same scenario, in which all tasks share similar instructions.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">While a longer task chain with multiple examples would better approximate practical sequential evaluation processes on the whole test set, it would significantly increase training costs. For simplicity, this paper focuses on task chains containing only two examples. A detailed discussion of Sequential Rollout with extended task chains is provided in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A2" title="Appendix B SAGE with Longer Task Chain in Sequential Rollout ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Skill-integrated Reward</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Unlike baseline RL that relies on outcome-based rewards, our training framework incorporates additional rewards specifically designed for the agent‚Äôs interactions with the skill library. For this purpose, we introduce the Skill-integrated Reward. Specifically, we aim to encourage two additional behaviors across a two-example task chain: skill generation in the first example and skill utilization in the second example.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.6">Formally, let <math alttext="r^{1},r^{2}\in[0,1]" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.1.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>,</mo><msup><mi>r</mi><mn>2</mn></msup></mrow><mo>‚àà</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">r^{1},r^{2}\in[0,1]</annotation></semantics></math> denote the verifiable outcome-based rewards of the task chain <math alttext="(q^{1},q^{2})" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q^{1},q^{2})</annotation></semantics></math> collected from Sequential Rollout, where the skills generated by <math alttext="q^{1}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.3.m3" intent=":literal"><semantics><msup><mi>q</mi><mn>1</mn></msup><annotation encoding="application/x-tex">q^{1}</annotation></semantics></math> are directly utilized by <math alttext="q^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.4.m4" intent=":literal"><semantics><msup><mi>q</mi><mn>2</mn></msup><annotation encoding="application/x-tex">q^{2}</annotation></semantics></math>. We formulate Skill-integrated Rewards <math alttext="R^{1}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.5.m5" intent=":literal"><semantics><msup><mi>R</mi><mn>1</mn></msup><annotation encoding="application/x-tex">R^{1}</annotation></semantics></math> and <math alttext="R^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.6.m6" intent=":literal"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding="application/x-tex">R^{2}</annotation></semantics></math> as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{cases}R^{1}=r^{1}+\mathbf{1}[r^{1}=1]*\mathbf{1}[r^{2}=1]*\mathbf{1}_{skill}(q^{2}|q^{1})\\
R^{2}=r^{2}+\mathbf{1}[r^{2}=1]*\mathbf{1}_{skill}(q^{2}|q^{1})\end{cases}" class="ltx_Math" display="block" id="S3.Ex4.m1" intent=":literal"><semantics><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><msup><mi>R</mi><mn>1</mn></msup><mo>=</mo><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>+</mo><mrow><mrow><mrow><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><mn>ùüè</mn></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><msub><mn>ùüè</mn><mrow><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>k</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>q</mi><mn>2</mn></msup><mo fence="false">|</mo><msup><mi>q</mi><mn>1</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>+</mo><mrow><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><msub><mn>ùüè</mn><mrow><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>k</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>q</mi><mn>2</mn></msup><mo fence="false">|</mo><msup><mi>q</mi><mn>1</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mtd><mtd></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{cases}R^{1}=r^{1}+\mathbf{1}[r^{1}=1]*\mathbf{1}[r^{2}=1]*\mathbf{1}_{skill}(q^{2}|q^{1})\\
R^{2}=r^{2}+\mathbf{1}[r^{2}=1]*\mathbf{1}_{skill}(q^{2}|q^{1})\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS3.p2.10">where indicator <math alttext="\mathbf{1}_{skill}(q^{2}|q^{1})" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.7.m1" intent=":literal"><semantics><mrow><msub><mn>ùüè</mn><mrow><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>k</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>q</mi><mn>2</mn></msup><mo fence="false">|</mo><msup><mi>q</mi><mn>1</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{1}_{skill}(q^{2}|q^{1})</annotation></semantics></math> denotes whether <math alttext="q^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.8.m2" intent=":literal"><semantics><msup><mi>q</mi><mn>2</mn></msup><annotation encoding="application/x-tex">q^{2}</annotation></semantics></math> uses skills generated by <math alttext="q^{1}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.9.m3" intent=":literal"><semantics><msup><mi>q</mi><mn>1</mn></msup><annotation encoding="application/x-tex">q^{1}</annotation></semantics></math>; and <math alttext="\mathbf{1}[r=1]" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.10.m4" intent=":literal"><semantics><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{1}[r=1]</annotation></semantics></math> represents whether the outcome-based reward equals 1, i.e. successful task completion. To ensure the skill library agent adheres to a specific format that generates code for each interaction, we impose a -1.0 penalty reward specifically on responses where the agent provides no code and terminates the task.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>SAGE</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">After collecting the rollout trajectories and computing the corresponding rewards, we implement our Skill Augmented GRPO for self-Evolution (SAGE) to train the skill library agent. Following <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span></cite>, we choose not to use the KL divergence penalty and the advantage would not be normalized by standard deviation of the rewards.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS4.p2">
<p class="ltx_p" id="S3.SS2.SSS4.p2.12">Given the current policy <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.1.m1" intent=":literal"><semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> and the old policy <math alttext="\pi_{\theta_{old}}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.2.m2" intent=":literal"><semantics><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>d</mi></mrow></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{old}}</annotation></semantics></math>, we first sample task chains from the original data distribution: <math alttext="(q^{1},q^{2})\sim Q" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.3.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><mo>‚àº</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">(q^{1},q^{2})\sim Q</annotation></semantics></math>. For each task pair, we then perform <math alttext="G" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.4.m4" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> group rollout <math alttext="\{\tau_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot|(q^{1},q^{2}))" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS4.p2.5.m5" intent=":literal"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>œÑ</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mo>‚àº</mo><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext>old</mtext></msub></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\{\tau_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot|(q^{1},q^{2}))</annotation></semantics></math>, where <math alttext="\tau_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.6.m6" intent=":literal"><semantics><msub><mi>œÑ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\tau_{i}</annotation></semantics></math> represents the Sequential Rollout trajectory collected by sequentially processing <math alttext="(q^{1},q^{2})" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.7.m7" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q^{1},q^{2})</annotation></semantics></math>. To differentiate outputs for each task query in <math alttext="\tau_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.8.m8" intent=":literal"><semantics><msub><mi>œÑ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\tau_{i}</annotation></semantics></math>, we define <math alttext="o_{i}^{k}\in\tau_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.9.m9" intent=":literal"><semantics><mrow><msubsup><mi>o</mi><mi>i</mi><mi>k</mi></msubsup><mo>‚àà</mo><msub><mi>œÑ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">o_{i}^{k}\in\tau_{i}</annotation></semantics></math> as the <math alttext="i^{\text{th}}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.10.m10" intent=":literal"><semantics><msup><mi>i</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">i^{\text{th}}</annotation></semantics></math> output for <math alttext="q^{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.11.m11" intent=":literal"><semantics><msup><mi>q</mi><mi>k</mi></msup><annotation encoding="application/x-tex">q^{k}</annotation></semantics></math> in the group, where <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.12.m12" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> is the chain index. The current policy is then optimized by maximizing the following objective function for skill library-integrated GRPO:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A12.EGx2">
<tbody id="S3.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{J}_{Agent}(\theta)" class="ltx_Math" display="inline" id="S3.Ex5.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ùí•</mi><mrow><mi>A</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>g</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>n</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>t</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>Œ∏</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{J}_{Agent}(\theta)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathbb{E}_{{\color[rgb]{1,0,0}(q^{1},q^{2})\sim Q,\{\tau_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot|(q^{1},q^{2}))}}" class="ltx_math_unparsed" display="inline" id="S3.Ex5.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><msub><mi>ùîº</mi><mrow><mrow><mo mathcolor="#FF0000" stretchy="false" style="--ltx-fg-color:#FF0000;">(</mo><msup><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">q</mi><mn mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">1</mn></msup><mo mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">,</mo><msup><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">q</mi><mn mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">2</mn></msup><mo mathcolor="#FF0000" stretchy="false" style="--ltx-fg-color:#FF0000;">)</mo></mrow><mo mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">‚àº</mo><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">Q</mi><mo mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">,</mo><msubsup><mrow><mo mathcolor="#FF0000" stretchy="false" style="--ltx-fg-color:#FF0000;">{</mo><msub><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">œÑ</mi><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">i</mi></msub><mo mathcolor="#FF0000" stretchy="false" style="--ltx-fg-color:#FF0000;">}</mo></mrow><mrow><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">i</mi><mo mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">=</mo><mn mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">1</mn></mrow><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">G</mi></msubsup><mo mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">‚àº</mo><msub><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">œÄ</mi><msub><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">Œ∏</mi><mtext mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">old</mtext></msub></msub><mrow><mo mathcolor="#FF0000" stretchy="false" style="--ltx-fg-color:#FF0000;">(</mo><mo lspace="0em" mathcolor="#FF0000" rspace="0em" style="--ltx-fg-color:#FF0000;">‚ãÖ</mo><mo fence="false" mathcolor="#FF0000" rspace="0.167em" stretchy="false" style="--ltx-fg-color:#FF0000;">|</mo><mrow><mo mathcolor="#FF0000" stretchy="false" style="--ltx-fg-color:#FF0000;">(</mo><msup><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">q</mi><mn mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">1</mn></msup><mo mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">,</mo><msup><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">q</mi><mn mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">2</mn></msup><mo mathcolor="#FF0000" stretchy="false" style="--ltx-fg-color:#FF0000;">)</mo></mrow><mo mathcolor="#FF0000" stretchy="false" style="--ltx-fg-color:#FF0000;">)</mo></mrow></mrow></msub></mrow><annotation encoding="application/x-tex">\displaystyle=\mathbb{E}_{{\color[rgb]{1,0,0}(q^{1},q^{2})\sim Q,\{\tau_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot|(q^{1},q^{2}))}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\frac{1}{G}\sum_{i=1}^{G}{\color[rgb]{1,0,0}\sum_{k=1}^{2}}\frac{1}{|o_{i}^{k}|}\sum_{t=1}^{|o_{i}^{k}|}\{\min[s_{i,t}^{k}\hat{A}_{i}^{k}," class="ltx_math_unparsed" display="inline" id="S3.Ex6.m1" intent=":literal"><semantics><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>G</mi></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo mathcolor="#FF0000" movablelimits="false" style="--ltx-fg-color:#FF0000;">‚àë</mo><mrow><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">k</mi><mo mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">=</mo><mn mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">1</mn></mrow><mn mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">2</mn></munderover></mstyle><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><msubsup><mi>o</mi><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><msubsup><mi>o</mi><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">|</mo></mrow></munderover></mstyle><mrow><mo stretchy="false">{</mo><mi>min</mi><mrow><mo stretchy="false">[</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi><mi>k</mi></msubsup><mo>,</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\frac{1}{G}\sum_{i=1}^{G}{\color[rgb]{1,0,0}\sum_{k=1}^{2}}\frac{1}{|o_{i}^{k}|}\sum_{t=1}^{|o_{i}^{k}|}\{\min[s_{i,t}^{k}\hat{A}_{i}^{k},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\text{clip}\left(s_{i,t}^{k},1-\epsilon,1+\epsilon\right)\hat{A}_{i}^{k}]\}" class="ltx_math_unparsed" display="inline" id="S3.Ex7.m1" intent=":literal"><semantics><mrow><mrow><mtext>clip</mtext><mrow><mo>(</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>œµ</mi><mo>)</mo></mrow><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">]</mo></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\displaystyle\text{clip}\left(s_{i,t}^{k},1-\epsilon,1+\epsilon\right)\hat{A}_{i}^{k}]\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS4.p2.24">where <math alttext="s_{i,t}^{k}=\frac{\pi_{\theta}(o_{i,t}^{k}|q^{k},{\color[rgb]{1,0,0}\mathcal{M}_{i}^{k}},o_{i,&lt;t}^{k})}{\pi_{\theta_{old}}(o_{i,t}^{k}|q^{k},{\color[rgb]{1,0,0}\mathcal{M}_{i}^{k}},o_{i,&lt;t}^{k}))}" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS4.p2.13.m1" intent=":literal"><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><mo>=</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><mo fence="false">|</mo><mrow><msup><mi>q</mi><mi>k</mi></msup><mo>,</mo><msubsup><mi class="ltx_font_mathcaligraphic" mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">‚Ñ≥</mi><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">i</mi><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">k</mi></msubsup><mo>,</mo><msubsup><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow><mi>k</mi></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>d</mi></mrow></msub></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>q</mi><mi>k</mi></msup><mo>,</mo><msubsup><mi class="ltx_font_mathcaligraphic" mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">‚Ñ≥</mi><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">i</mi><mi mathcolor="#FF0000" style="--ltx-fg-color:#FF0000;">k</mi></msubsup><mo>,</mo><msubsup><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow><mi>k</mi></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">s_{i,t}^{k}=\frac{\pi_{\theta}(o_{i,t}^{k}|q^{k},{\color[rgb]{1,0,0}\mathcal{M}_{i}^{k}},o_{i,&lt;t}^{k})}{\pi_{\theta_{old}}(o_{i,t}^{k}|q^{k},{\color[rgb]{1,0,0}\mathcal{M}_{i}^{k}},o_{i,&lt;t}^{k}))}</annotation></semantics></math>; and <math alttext="\hat{A}_{i}^{k}=R_{i}^{k}-\text{mean}(\{R_{i}^{k}|i=1,2,..,G\})" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS4.p2.14.m2" intent=":literal"><semantics><mrow><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi><mi>k</mi></msubsup><mo>=</mo><msubsup><mi>R</mi><mi>i</mi><mi>k</mi></msubsup><mo>‚àí</mo><mtext>mean</mtext><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">{</mo><msubsup><mi>R</mi><mi>i</mi><mi>k</mi></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo lspace="0em" rspace="0.0835em">.</mo><mo lspace="0.0835em" rspace="0.167em">.</mo><mo>,</mo><mi>G</mi><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{A}_{i}^{k}=R_{i}^{k}-\text{mean}(\{R_{i}^{k}|i=1,2,..,G\})</annotation></semantics></math>. <math alttext="s_{i,t}^{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.15.m3" intent=":literal"><semantics><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><annotation encoding="application/x-tex">s_{i,t}^{k}</annotation></semantics></math> represents the importance sampling term; <math alttext="\hat{A}_{i}^{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.16.m4" intent=":literal"><semantics><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">\hat{A}_{i}^{k}</annotation></semantics></math> is the advantage computed by the Skill-integrated Reward <math alttext="R_{i}^{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.17.m5" intent=":literal"><semantics><msubsup><mi>R</mi><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">R_{i}^{k}</annotation></semantics></math>; <math alttext="\mathcal{M}_{i}^{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.18.m6" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">\mathcal{M}_{i}^{k}</annotation></semantics></math> denotes the skill library integrated with the query <math alttext="q_{i}^{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.19.m7" intent=":literal"><semantics><msubsup><mi>q</mi><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">q_{i}^{k}</annotation></semantics></math> for group <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.20.m8" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>. Here <math alttext="\mathcal{M}_{i}^{1}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.21.m9" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><mi>i</mi><mn>1</mn></msubsup><annotation encoding="application/x-tex">\mathcal{M}_{i}^{1}</annotation></semantics></math> is an empty set and <math alttext="\mathcal{M}_{i}^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.22.m10" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><mi>i</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\mathcal{M}_{i}^{2}</annotation></semantics></math> includes skills generated when performing <math alttext="q_{i}^{1}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.23.m11" intent=":literal"><semantics><msubsup><mi>q</mi><mi>i</mi><mn>1</mn></msubsup><annotation encoding="application/x-tex">q_{i}^{1}</annotation></semantics></math>. Since our skill library agents need multi-turn interactions with the environments to process the task, the outputs <math alttext="|o_{i}^{k}|" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p2.24.m12" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><msubsup><mi>o</mi><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|o_{i}^{k}|</annotation></semantics></math> only consider the LLM generation contents, while the observations from the environment are masked.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS4.p3">
<p class="ltx_p" id="S3.SS2.SSS4.p3.2">The key differences between our approach and the original GRPO described in Section 3.2.1 are highlighted in red. In our method, due to the Sequential Rollout mechanism, the expectation is computed across the task chain. Notably, within the same group, the generations <math alttext="o_{i}^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p3.1.m1" intent=":literal"><semantics><msubsup><mi>o</mi><mi>i</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">o_{i}^{2}</annotation></semantics></math> are derived from different skill libraries <math alttext="\mathcal{M}_{i}^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS4.p3.2.m2" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><mi>i</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\mathcal{M}_{i}^{2}</annotation></semantics></math>, unlike the original GRPO where generations stem from identical queries.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We begin this section with the details of our experimental settings followed by the presentation of our main results. Further analysis and ablation studies are conducted to better demonstrate the effectiveness of SAGE.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Settings</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Dataset and Base Model.</span> To evaluate the effectiveness of SAGE for skill library agents, particularly for those designed for long-horizon tool usage, we adopt the AppWorld dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">trivedi2024appworld</span>)</cite>. AppWorld contains 750 tasks from 250 task scenarios. Each scenario consists of three tasks sharing similar instructions. The scenario-based structure of AppWorld makes it well-suited for Sequential Rollout, as tasks within the same scenario naturally form a task chain. We utilize the Train set for all training steps, including SFT and subsequent SAGE. The Dev set guides the best checkpoint selection during training. Evaluation is performed on both Test-Normal and Test-Challenge splits. More details about the dataset are presented in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A3" title="Appendix C AppWorld Dataset ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To ensure a fair comparison with the previous work <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span>)</cite> on AppWorld, we used Qwen2.5-32B-Instruct <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">qwen2.5</span>)</cite> as our base model for training purpose.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Skill Library Agent.</span> Following the ReAct agent <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">yao2023react</span>)</cite> as presented in the AppWorld paper <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">trivedi2024appworld</span>)</cite>, we implemented our skill library agent with a specifically designed in-context example and detailed instructions to guide the base model in using the skill library to perform complex tasks. The detailed prompt of our skill library agent is presented in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A4" title="Appendix D Prompt for Skill Library Agent ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">D</span></a>. Regarding skill retrieval, we employed an idealized case during sequential rollout where skills are retained and utilized only within the same scenario in the task chain. This approach eliminates the need for a retrieval model, significantly simplifying the RL rollout processes. While our default evaluation uses the ideal case (Same Scenario) with provided scenario labels from the test set, we conducted additional analysis on various retrieval methods to address real-world scenarios where such labels may not be available.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Baseline GRPO.</span> Since the baseline method LOOP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span>)</cite> does not provide source code or model, and their paper does not report agent efficiency metrics, it is challenging to make a direct, fair comparison. Therefore, we implemented our own baseline training using the GRPO method without Skill Library for comparison purposes. Following <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span></cite>, our baseline GRPO removes the KL divergence penalty and calculates advantage using the mean reward within the group instead of normalizing by the standard error. The only difference between our baseline GRPO and LOOP is that we perform strictly on-policy RL without PPO epochs. More training settings of Baseline GRPO are presented in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A5" title="Appendix E Training Details of Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">SAGE.</span> Initial RL experiments with open-source models revealed their limited capabilities in following instructions when integrated with our skill library agent, despite carefully designed prompts. This limitation resulted in insufficient self-improvement capabilities, hampering the generation of high-quality sequential rollouts necessary for our SAGE. Thus, we first implemented supervised fine-tuning prior to RL using an expert experience dataset. For simplicity, we employed an advanced model, specifically Claude 3.5 Sonnet V2, as the expert to produce high-quality trajectories within the skill library agent. The experimental details of expert data generation for SFT are shown in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A6" title="Appendix F Expert Experience Dataset Generation ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">F</span></a>. Based on the SFT model, we then performed our SAGE with sequential rollout and skill-integrated reward. Details of training parameters and processes are listed in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A7" title="Appendix G Training Details of SAGE ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">Metrics and Evaluation.</span>
We assessed performance using two primary metrics: <span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.2">Task Goal Completion (TGC)</span> measures the accuracy of successfully completed individual tasks; and <span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.3">Scenario Goal Completion (SGC)</span> calculates the proportion of scenarios where all three included tasks are successfully completed. To evaluate efficiency, we counted average interaction steps (<span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.4">Avg. Steps</span>) and average generated tokens (<span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.5">Avg. Tokens</span>) required for task completion, where fewer steps and tokens indicate more efficiency. All reported numbers in this paper represent the average of three agent evaluation runs using the same model.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Task Performance of SAGE compared with various baselines. Results of Methods marked with "*" are taken from <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span>)</cite>. In these cases, Avg. Steps and Tokens were not reported, thus denoted by "- -".</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:910.6pt;height:214.1pt;vertical-align:-103.9pt;"><span class="ltx_transformed_inner" style="transform:translate(90.4pt,-21.2pt) scale(1.24759516957146,1.24759516957146) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.1.1.1.1">Base Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.1.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.1.1.2.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T1.1.1.1.1.3">Test Normal</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T1.1.1.1.1.4">Test Challenge</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.2.2">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.1">TGC</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.2">SGC</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.3">Avg. Steps</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.4">Avg. Tokens</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.5">TGC</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.6">SGC</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.7">Avg. Steps</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2.2.8">Avg. Token</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="10" id="S4.T1.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.3.1.1">Training Free Methods</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.1">GPT-4o</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.2">ReAct*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.3">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.4">32.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.5">- -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.6">- -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.7">30.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.8">13.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.9">- -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.10">- -</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.5">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.1">OpenAI o1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.2">ReAct*</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.3">61.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.4">41.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.5">- -</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.6">- -</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.7">36.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.8">19.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.9">- -</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.5.10">- -</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.6">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.1">Claude Sonnet 3.5 V2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.2">ReAct</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.3">57.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.4">41.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.5">15.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.6">1,542</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.7">49.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.8">28.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.9">21.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6.10">2,084</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.7">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.1">Qwen2.5 32B Instruct</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.2">ReAct*</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.3">39.2 ¬± 3.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.4">18.6 ¬± 2.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.5">- -</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.6">- -</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.7">21.0 ¬± 1.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.8">7.5 ¬± 1.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.9">- -</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7.10">- -</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="10" id="S4.T1.1.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.8.8.1.1">RL without Skill Library</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.9.9.1.1">Qwen2.5 32B Instruct</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.2">LOOP*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.3">71.3 ¬± 1.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.4">53.6 ¬± 2.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.5">- -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.6">- -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.7">45.7 ¬± 1.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.8">26.6 ¬± 1.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.9">- -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.10">- -</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.10.10">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.1">GRPO</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.2">69.2 ¬± 2.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.3">51.8 ¬± 5.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.4">16.4 ¬± 0.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.5">3,613 ¬± 200</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.6">40.7 ¬± 1.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.7">26.9 ¬± 1.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.8">21.9 ¬± 0.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.10.9">5,211 ¬± 65</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="10" id="S4.T1.1.1.11.11.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.11.11.1.1">Our Approach</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.1.12.12.1" rowspan="3"><span class="ltx_text" id="S4.T1.1.1.12.12.1.1">Qwen2.5 32B Instruct</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.12.2">Skill Library Agent</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.12.12.3">30.7 ¬± 3.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.12.12.4">19.6 ¬± 1.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.12.12.5">13.4 ¬± 0.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.12.12.6">2,988 ¬± 73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.12.12.7">15.3 ¬± 1.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.12.12.8">7.0 ¬± 1.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.12.12.9">18.7 ¬± 0.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.12.12.10">4,803 ¬± 117</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.13">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.13.13.1">+ SFT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13.2">55.2 ¬± 1.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13.3">41.7 ¬± 1.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.13.13.4.1">11.4 ¬± 0.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.13.13.5.1">1,340 ¬± 65</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13.6">37.2 ¬± 1.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13.7">20.9 ¬± 1.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13.8"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.13.13.8.1">16.2 ¬± 0.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13.9">1,909 ¬± 80</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.14.14">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.1.14.14.1">+ SAGE</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.14.14.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.14.14.2.1">72.0 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.14.14.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.14.14.3.1">60.7 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.14.14.4">12.1 ¬± 0.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.14.14.5">1,475 ¬± 127</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.14.14.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.14.14.6.1">50.1 ¬± 2.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.14.14.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.14.14.7.1">32.4 ¬± 3.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.14.14.8">17.3 ¬± 0.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.14.14.9"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.14.14.9.1">1,807 ¬± 29</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.T1" title="Table 1 ‚Ä£ 4.1 Experimental Settings ‚Ä£ 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">1</span></a> presents our main results of SAGE compared with various baselines. From the table, we can observe that SAGE demonstrates significantly improved performance compared to the baseline GRPO, particularly a 8.9% improvement in SGC (Scenario Goal Completion) on test normal dataset. Since SGC measures the agent‚Äôs performance across multiple related tasks within a scenario, this improvement demonstrates our model‚Äôs ability to effectively transfer and reuse skills across similar tasks. Additionally, skill library agent trained by SAGE significantly reduces average interaction steps and generated tokens, with 59% less tokens compared to the baseline agent trained by GRPO. This demonstrates that skill reuse can accomplish complex tasks more efficiently at lower cost. Although our final results rely on SFT using expert experience data generated by Claude, our RL approach with the skill library enables open-source models to surpass the expert performance.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">For the results of our approach, we presented the findings in a stepwise manner to better demonstrate the improvements achieved at each stage (prompting based skill library agent, SFT, and SAGE). Initially, when compared to the baseline training-free approach using the Qwen2.5 32B Instruction model with ReAct agent, our skill library agent shows lower performance, indicating the limitations of prompt-based methods for self-improving agent with skill library. The performance significantly improves after SFT with expert experience generated by Claude, though still not surpassing the baseline GRPO without skill library. This indicates that merely imitating expert behaviors is insufficient for optimized performance. Ultimately, our RL method further enhances the SFT-trained model, achieving superior performance compared to all baselines. To better understand how our approach enhances task performance with skill library, we provide examples of actual task execution across different agent models in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A8" title="Appendix H Task Execution Examples with Different Models ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">H</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Skill Library Usage Analysis</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Though SAGE has reached state-of-the-art performance on AppWorld, it remains unclear how skill library are actually involved during evaluation. This section provides a detailed analysis of skill library usage patterns at each stage of our approach. We employ various metrics to evaluate skill library utilization during evaluation. (1) <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Skill Usage Rate</span>: Among examples with skill library, the proportion that use skills; (2) <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.2">Success Skill Usage Rate</span>: Among examples that use skills, the proportion that reach successful task completion; (3) <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.3">Skill Library Size</span>: Total number of generated skills in the skill library; (4) <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.4">Used Skill Num</span>: Number of skills in the skill library being used. The analysis results are summarized in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.F2" title="Figure 2 ‚Ä£ 4.3 Skill Library Usage Analysis ‚Ä£ 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="119" id="S4.F2.g1" src="model_comparison_chart.png" width="238"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Analysis of Skill Usage Patterns. Performance metrics are shown as ratios relative to Base Model baseline, with numerical values annotated.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Analysis of the results reveals that SAGE significantly improves both Skill Usage Rate and Success Skill Usage Rate compared to previous steps. This improvement demonstrates enhanced skill utilization and self-improving capabilities developed during the RL process. The Base Model shows high Skill Usage Rate and Skill Library Size, indicating its fundamental ability to generate and use skills in response to instructional prompts. Although the Base Model generates more skills than SAGE, its lower Used Skill Num and Success Skill Usage Rate indicate limitations in both skill generation quality and utilization effectiveness. Notably, the SFT model only surpasses the Base Model in Success Skill Usage Rate. This outcome may be attributed to the limitations of SFT from expert experiences. While these experiences enhance overall performance, they appear insufficient for developing the model‚Äôs self-improvement capabilities in skill generation and usage.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To further demonstrate the effectiveness of SAGE, we perform ablation studies shown as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Evaluation without Skills.</span> To validate that the inclusion of the skill library indeed contributes to better performance, we conducted additional evaluations of the skill library agent but with an empty skill library, on the test normal dataset.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Stepwise performance of skill library agent compared with no skills involved.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.3" style="width:433.6pt;height:110.9pt;vertical-align:-52.6pt;"><span class="ltx_transformed_inner" style="transform:translate(24.4pt,-6.2pt) scale(1.12677421205617,1.12677421205617) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.3.3.4.1.1" rowspan="2"><span class="ltx_text" id="S4.T2.3.3.4.1.1.1">Step</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.3.3.4.1.2" rowspan="2"><span class="ltx_text" id="S4.T2.3.3.4.1.2.1">With Skill</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T2.3.3.4.1.3">Test Normal</th>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.5.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.5.2.1">TGC</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.5.2.2">SGC</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.5.2.3">Avg. Steps</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.5.2.4">Avg. Tokens</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.3.3.6.3.1">Skill Library</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.3.3.6.3.2">‚úì</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.6.3.3">30.7 ¬± 3.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.6.3.4">19.6 ¬± 1.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.6.3.5">13.4 ¬± 0.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.6.3.6">2,988 ¬± 73</th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.1.1.2">Agent</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.1.1.1"><math alttext="\bm{\times}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1" intent=":literal"><semantics><mo class="ltx_mathvariant_bold" mathvariant="bold">√ó</mo><annotation encoding="application/x-tex">\bm{\times}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.3">34.7 ¬± 3.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.4">14.9 ¬± 3.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.5">16.4 ¬± 0.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.6">3,704 ¬± 139</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.7.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.3.3.7.4.1" rowspan="2"><span class="ltx_text" id="S4.T2.3.3.7.4.1.1">SFT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.3.3.7.4.2">‚úì</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.7.4.3">55.2 ¬± 1.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.7.4.4">41.7 ¬± 1.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.7.4.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.7.4.5.1">11.4 ¬± 0.5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.7.4.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.7.4.6.1">1,340 ¬± 65</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.2.2.2.1"><math alttext="\bm{\times}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.m1" intent=":literal"><semantics><mo class="ltx_mathvariant_bold" mathvariant="bold">√ó</mo><annotation encoding="application/x-tex">\bm{\times}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2.2">54.8 ¬± 2.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2.3">39.9 ¬± 2.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2.4">13.5 ¬± 0.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2.5">1,611 ¬± 11</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.8.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.3.3.8.5.1" rowspan="2"><span class="ltx_text" id="S4.T2.3.3.8.5.1.1">SAGE</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.3.3.8.5.2">‚úì</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.8.5.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.8.5.3.1">72.0 ¬± 1.5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.8.5.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.8.5.4.1">60.7 ¬± 1.5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.8.5.5">12.1 ¬± 0.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.3.8.5.6">1,475 ¬± 127</th>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T2.3.3.3.1"><math alttext="\bm{\times}" class="ltx_Math" display="inline" id="S4.T2.3.3.3.1.m1" intent=":literal"><semantics><mo class="ltx_mathvariant_bold" mathvariant="bold">√ó</mo><annotation encoding="application/x-tex">\bm{\times}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3.2">71.4 ¬± 0.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3.3">54.8 ¬± 0.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3.4">16.0 ¬± 0.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3.5">1,937 ¬± 81</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">As shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.T2" title="Table 2 ‚Ä£ 4.4 Ablation Studies ‚Ä£ 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">2</span></a>, all agent models achieve improved SGC scores with reduced average steps and tokens when employing skills compared to no skill settings. This improvement highlights the importance of incorporating the skill library to enhance SGC performance and task efficiency, even in the absence of training. However, we observe a decline in TGC scores for Skill Library Agent. This decline may be attributed to the models‚Äô limited proficiency in skill utilization, resulting in inappropriate skill applications and consequent task failures, as detailed in Section 4.3.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">Skill Library Agent with Retrieval in Practice.</span>
In practical applications, tasks often lack explicit scenario information that would enable direct skill acquisition from the same scenario as used during training. To address this limitation, we implemented an additional retrieval process that identifies and leverages relevant skills from previous experiences during evaluation. Specifically, we explored three different retrieval methods: Query N-gram, Query Embedding, and Skill Embedding.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>SAGE with different retrieval methods.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:89.9pt;vertical-align:-41.9pt;"><span class="ltx_transformed_inner" style="transform:translate(40.2pt,-8.3pt) scale(1.22790341872801,1.22790341872801) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.1.1.1.1">Retrieval Methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T3.1.1.1.1.2">Test Normal</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.2.2.1">TGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.2.2.2">SGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.2.2.3">Avg. Steps</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.2.2.4">Avg. Tokens</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3.1.1">Same Scenario</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.1.2.1">72.0 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.1.3.1">60.7 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3.1.4">12.1 ¬± 0.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3.1.5">1,475 ¬± 127</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4.2">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.2.1">Query N-gram</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.2.2.1">72.0 ¬± 2.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.2.3">60.1 ¬± 1.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.2.4">12.7 ¬± 0.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.2.5">1,466 ¬± 101</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5.3">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.3.1">Query Embedding</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.3.2">69.6 ¬± 1.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.3.3">59.5 ¬± 2.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.3.4.1">11.8 ¬± 0.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.3.5.1">1,335 ¬± 63</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.6.4.1">Skill Embedding</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.6.4.2">66.3 ¬± 0.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.6.4.3">56.0 ¬± 0.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.6.4.4">14.5 ¬± 0.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.6.4.5">1,692 ¬± 10</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">The results in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.T3" title="Table 3 ‚Ä£ 4.4 Ablation Studies ‚Ä£ 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">3</span></a> show that retrieval method selection significantly impacts performance. Carefully designed skill retrieval mechanisms can approach or even partially outperform the best performance achieved under Same Scenario conditions.
A comprehensive description of each retrieval method, along with detailed results analysis, can be found in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A9" title="Appendix I Retrieval Method Ablation Study ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p6.1.1">Reward Design.</span> To demonstrate the effectiveness of our Skill-integrated Reward design, we conducted ablation studies comparing it with two alternative reward designs: Outcome-based Reward and Chain-based Reward. Details of the alternative reward designs and their corresponding formulations are included in the Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A10.SS1" title="J.1 Details of Reward Designs ‚Ä£ Appendix J Reward Design Ablation Study ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">J.1</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>SAGE with different reward designs.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:433.6pt;height:78pt;vertical-align:-35.8pt;"><span class="ltx_transformed_inner" style="transform:translate(46.9pt,-8.4pt) scale(1.27566705973712,1.27566705973712) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T4.1.1.1.1.1.1">Reward Design</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T4.1.1.1.1.2">Test Normal</th>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.2.2.1">TGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.2.2.2">SGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.2.2.3">Avg. Steps</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.2.2.4">Avg. Tokens</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.1.1">Skill-integrated</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.1.2.1">72.0 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.1.3.1">60.7 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.1.4.1">12.1 ¬± 0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.1.5">1,475 ¬± 127</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.4.2">
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.2.1">Outcome-based</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.2.2">69.8 ¬± 1.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.2.3">55.4 ¬± 1.4</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.2.4">13.1 ¬± 0.2</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.2.5">1,469 ¬± 61</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.5.3.1">Chain-based</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.5.3.2">67.9 ¬± 2.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.5.3.3">56.6 ¬± 1.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.5.3.4">15.7 ¬± 0.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.5.3.5.1">1,361 ¬± 58</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS4.p7">
<p class="ltx_p" id="S4.SS4.p7.1">As shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.T4" title="Table 4 ‚Ä£ 4.4 Ablation Studies ‚Ä£ 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">4</span></a>, our Skill-integrated Reward design achieves superior TGC and SGC scores compared to alternative reward designs. Regarding efficiency, while Skill-integrated Reward yields the lowest Avg. Seps, it results in relatively high Avg. Tokens. This trade-off likely stems from skill usage reducing the number of steps while requiring additional tokens for skill generation. We further analyze the skill library usage behaviors across different reward designs through additional studies (similar to Section 4.3) in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A10.SS2" title="J.2 Skill Library Usage Analysis ‚Ä£ Appendix J Reward Design Ablation Study ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">J.2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p8">
<p class="ltx_p" id="S4.SS4.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p8.1.1">RL Initialization.</span> In this paper, we initialized SAGE using SFT. To demonstrate the necessity of using extra data for SFT before RL, we conducted ablation studies with various initialization methods including Base Model, Self-Distillation and RL Warm-Up (detailed in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A11" title="Appendix K RL Initialization Methods ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">K</span></a>).</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>SAGE initialized with different methods.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.3.3" style="width:433.6pt;height:83.1pt;vertical-align:-38.7pt;"><span class="ltx_transformed_inner" style="transform:translate(25.7pt,-4.9pt) scale(1.13463772307132,1.13463772307132) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.3.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.3.3.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.3.3.3.4.1.1">Initialization</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.3.3.3.4.1.2">Extra</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T5.3.3.3.4.1.3">Test Normal</th>
</tr>
<tr class="ltx_tr" id="S4.T5.3.3.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T5.3.3.3.5.2.1">Methods</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T5.3.3.3.5.2.2">Data</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.3.3.3.5.2.3">TGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.3.3.3.5.2.4">SGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.3.3.3.5.2.5">Avg. Steps</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.3.3.3.5.2.6">Avg. Tokens</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.1.1.1.2">Base Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.1.1.1.1"><math alttext="\bm{\times}" class="ltx_Math" display="inline" id="S4.T5.1.1.1.1.1.m1" intent=":literal"><semantics><mo class="ltx_mathvariant_bold" mathvariant="bold">√ó</mo><annotation encoding="application/x-tex">\bm{\times}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.1.1.3">40.7 ¬± 2.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.1.1.4">25.6 ¬± 0.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.5.1">11.9 ¬± 0.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.1.1.6">2,532 ¬± 93</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.2.2.2.2.2">Self Distillation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.2.2.2.2.1"><math alttext="\bm{\times}" class="ltx_Math" display="inline" id="S4.T5.2.2.2.2.1.m1" intent=":literal"><semantics><mo class="ltx_mathvariant_bold" mathvariant="bold">√ó</mo><annotation encoding="application/x-tex">\bm{\times}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.2.3">66.5 ¬± 1.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.2.4">53.6 ¬± 5.3</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.2.5">13.1 ¬± 0.4</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.2.6">2,321 ¬± 103</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.3.3.3.3.2">RL Warm-Up</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.3.3.3.3.1"><math alttext="\bm{\times}" class="ltx_Math" display="inline" id="S4.T5.3.3.3.3.1.m1" intent=":literal"><semantics><mo class="ltx_mathvariant_bold" mathvariant="bold">√ó</mo><annotation encoding="application/x-tex">\bm{\times}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.3.3.3">68.3 ¬± 0.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.3.3.4">55.3 ¬± 3.8</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.3.3.5">16.0 ¬± 0.3</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.3.3.6">2,556 ¬± 62</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.3.3.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T5.3.3.3.6.1.1">SFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T5.3.3.3.6.1.2">‚úì</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.3.3.6.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.3.3.3.6.1.3.1">72.0 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.3.3.6.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.3.3.3.6.1.4.1">60.7 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.3.3.6.1.5">12.1 ¬± 0.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.3.3.3.6.1.6"><span class="ltx_text ltx_font_bold" id="S4.T5.3.3.3.6.1.6.1">1,475 ¬± 127</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS4.p9">
<p class="ltx_p" id="S4.SS4.p9.1">The comparative results in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.T5" title="Table 5 ‚Ä£ 4.4 Ablation Studies ‚Ä£ 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrate that SAGE, when initialized with SFT on the expert experience dataset, significantly outperforms other approaches. This underscores the crucial role of expert trajectories in guiding the agent model towards state-of-the-art performance while maintaining high efficiency. Among the methods without extra data, RL Warm-up shows better performance compared to Self Distillation, while training directly from the Base Model yields notably poor results. These findings further indicate the base model‚Äôs limited capability in skill library usage and its inadequacy for generating high-quality trajectories during rollouts without proper initialization.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p10">
<p class="ltx_p" id="S4.SS4.p10.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p10.1.1">SFT Initialized Baseline GRPO.</span>
For a fair comparison, we also conducted experiments using SFT to initialize baseline GRPO. Even with this setting, SAGE demonstrates significantly better performance. Detailed experimental settings, results, and analyses are presented in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12" title="Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">L</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper presents a pioneering work in exploring the application of RL to self-improving agents with skill library. To achieve this, we propose a novel RL framework named SAGE, which incorporates GRPO with Sequential Rollout and Skill-integrated Reward. When applied to AppWorld datasets, our approach enables the skill library agent to significantly outperform baselines in both performance and efficiency, paving the way for enhanced self-improvement capabilities with skill libraries through RL.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">In this paper, we conducted experiments exclusively on the AppWorld dataset to demonstrate the effectiveness of SAGE for skill library agent. We chose this dataset because its simulated environment more closely resembles real-world application scenarios. However, we acknowledge that different scenarios may require different agent designs, even when applying similar skill library approaches. In future work, we plan to extend our evaluation to other tool-using agent datasets.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Interaction Format Example of Skill Library Agent</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we provide examples of the interaction format for Skill Library Agent compared to the baseline agent on AppWorld. The following Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A1.F3" title="Figure 3 ‚Ä£ Appendix A Interaction Format Example of Skill Library Agent ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A1.F4" title="Figure 4 ‚Ä£ Appendix A Interaction Format Example of Skill Library Agent ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">4</span></a> present one interaction step of generating code to login the app Spotify for baseline agent and skill library agent respectively.</p>
</div>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="122" id="A1.F3.g1" src="appendix_a2.png" width="238"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The baseline agent directly generates codes to process the task.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="314" id="A1.F4.g1" src="appendix_a1.png" width="238"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Skill Library Agent will first define a function and then call it to process the task.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>SAGE with Longer Task Chain in Sequential Rollout</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">To help readers better understand our SAGE with Sequential Rollout and Skill-integrated Reward, we present a simple but effective case in Section 3.2 where only two example are involved for the task chain. In fact, SAGE has a general format which can be adapted to more than two tasks in the task chain of Sequential Rollout.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.4">Assuming that we sample <math alttext="K" class="ltx_Math" display="inline" id="A2.p2.1.m1" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> examples from the dataset to form the task chain, denoted as <math alttext="(q^{1},q^{2},..,q^{K})\sim Q" class="ltx_math_unparsed" display="inline" id="A2.p2.2.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo>,</mo><mo lspace="0em" rspace="0.0835em">.</mo><mo lspace="0.0835em" rspace="0.167em">.</mo><mo>,</mo><msup><mi>q</mi><mi>K</mi></msup><mo stretchy="false">)</mo></mrow><mo>‚àº</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">(q^{1},q^{2},..,q^{K})\sim Q</annotation></semantics></math>, the Sequential Rollout process is performed sequentially from <math alttext="q^{1}" class="ltx_Math" display="inline" id="A2.p2.3.m3" intent=":literal"><semantics><msup><mi>q</mi><mn>1</mn></msup><annotation encoding="application/x-tex">q^{1}</annotation></semantics></math> to <math alttext="q^{K}" class="ltx_Math" display="inline" id="A2.p2.4.m4" intent=":literal"><semantics><msup><mi>q</mi><mi>K</mi></msup><annotation encoding="application/x-tex">q^{K}</annotation></semantics></math> with a skill library, where skills are accumulated into the library and become available to use for all subsequent tasks within the task chain.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p3">
<p class="ltx_p" id="A2.p3.5">With the Sequential Rollout, here we present the general format for our Skill-integrated Reward. Let <math alttext="r^{1},r^{2},...,r^{K}\in[0,1]" class="ltx_Math" display="inline" id="A2.p3.1.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>,</mo><msup><mi>r</mi><mn>2</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>r</mi><mi>K</mi></msup></mrow><mo>‚àà</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">r^{1},r^{2},...,r^{K}\in[0,1]</annotation></semantics></math> be the verifiable outcome-based rewards of each task in the long task chain <math alttext="(q^{1},q^{2},...,q^{K})" class="ltx_Math" display="inline" id="A2.p3.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mi>K</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q^{1},q^{2},...,q^{K})</annotation></semantics></math>, the general format for the Skill-integrated Reward of the <math alttext="k^{\text{th}}" class="ltx_Math" display="inline" id="A2.p3.3.m3" intent=":literal"><semantics><msup><mi>k</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">k^{\text{th}}</annotation></semantics></math> task <math alttext="R^{k}" class="ltx_Math" display="inline" id="A2.p3.4.m4" intent=":literal"><semantics><msup><mi>R</mi><mi>k</mi></msup><annotation encoding="application/x-tex">R^{k}</annotation></semantics></math> (<math alttext="1&lt;k&lt;K" class="ltx_Math" display="inline" id="A2.p3.5.m5" intent=":literal"><semantics><mrow><mn>1</mn><mo>&lt;</mo><mi>k</mi><mo>&lt;</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">1&lt;k&lt;K</annotation></semantics></math>) can be formulated as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A12.EGx3">
<tbody id="A2.Ex8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R^{k}" class="ltx_Math" display="inline" id="A2.Ex8.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>k</mi></msup><annotation encoding="application/x-tex">\displaystyle R^{k}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=r^{k}" class="ltx_Math" display="inline" id="A2.Ex8.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><msup><mi>r</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\displaystyle=r^{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\mathbf{1}[r^{k}=1]*\max_{k&lt;i\leq K}(\mathbf{1}[r^{i}=1]*\mathbf{1}_{skill}(q^{i}|q^{k}))" class="ltx_Math" display="inline" id="A2.Ex9.m1" intent=":literal"><semantics><mrow><mo>+</mo><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mi>k</mi></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><mrow><munder><mi>max</mi><mrow><mi>k</mi><mo>&lt;</mo><mi>i</mi><mo>‚â§</mo><mi>K</mi></mrow></munder><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mi>i</mi></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><msub><mn>ùüè</mn><mrow><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>k</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>q</mi><mi>i</mi></msup><mo fence="false">|</mo><msup><mi>q</mi><mi>k</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle+\mathbf{1}[r^{k}=1]*\max_{k&lt;i\leq K}(\mathbf{1}[r^{i}=1]*\mathbf{1}_{skill}(q^{i}|q^{k}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\mathbf{1}[r^{k}=1]*\mathbf{1}_{skill}(q^{k}|q^{1},...,q^{k-1})" class="ltx_Math" display="inline" id="A2.Ex11.m1" intent=":literal"><semantics><mrow><mo>+</mo><mrow><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mi>k</mi></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><msub><mn>ùüè</mn><mrow><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>k</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>q</mi><mi>k</mi></msup><mo fence="false">|</mo><mrow><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle+\mathbf{1}[r^{k}=1]*\mathbf{1}_{skill}(q^{k}|q^{1},...,q^{k-1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.p3.12">where <math alttext="\mathbf{1}_{skill}(q^{k}|q^{1},...,q^{k-1})" class="ltx_Math" display="inline" id="A2.p3.6.m1" intent=":literal"><semantics><mrow><msub><mn>ùüè</mn><mrow><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>k</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>q</mi><mi>k</mi></msup><mo fence="false">|</mo><mrow><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{1}_{skill}(q^{k}|q^{1},...,q^{k-1})</annotation></semantics></math> evaluates whether the task <math alttext="q^{k}" class="ltx_Math" display="inline" id="A2.p3.7.m2" intent=":literal"><semantics><msup><mi>q</mi><mi>k</mi></msup><annotation encoding="application/x-tex">q^{k}</annotation></semantics></math> uses the skills generated from previous tasks <math alttext="q^{1},..q^{k-1}" class="ltx_math_unparsed" display="inline" id="A2.p3.8.m3" intent=":literal"><semantics><mrow><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><mo lspace="0em" rspace="0.0835em">.</mo><mo lspace="0.0835em" rspace="0.167em">.</mo><msup><mi>q</mi><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">q^{1},..q^{k-1}</annotation></semantics></math> in the task chain.
For the special cases of <math alttext="k=1" class="ltx_Math" display="inline" id="A2.p3.9.m4" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation></semantics></math> and <math alttext="k=K" class="ltx_Math" display="inline" id="A2.p3.10.m5" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">k=K</annotation></semantics></math>, the reward becomes slightly different. The first example (<math alttext="k=1" class="ltx_Math" display="inline" id="A2.p3.11.m6" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation></semantics></math>) begins with an empty skill library, thus cannot utilize previous skills. The last example (<math alttext="k=K" class="ltx_Math" display="inline" id="A2.p3.12.m7" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">k=K</annotation></semantics></math>) cannot have its generated skills verified by subsequent tasks. Therefore, we formulate these boundary cases as:</p>
</div>
<div class="ltx_para" id="A2.p4">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A12.EGx4">
<tbody id="A2.Ex12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R^{1}" class="ltx_Math" display="inline" id="A2.Ex12.m1" intent=":literal"><semantics><msup><mi>R</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\displaystyle R^{1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=r^{1}+\mathbf{1}[r^{1}=1]*\max_{1&lt;i\leq K}(\mathbf{1}[r^{i}=1]*\mathbf{1}_{skill}(q^{i}|q^{1}))" class="ltx_Math" display="inline" id="A2.Ex12.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>+</mo><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><mrow><munder><mi>max</mi><mrow><mn>1</mn><mo>&lt;</mo><mi>i</mi><mo>‚â§</mo><mi>K</mi></mrow></munder><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mi>i</mi></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><msub><mn>ùüè</mn><mrow><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>k</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>q</mi><mi>i</mi></msup><mo fence="false">|</mo><msup><mi>q</mi><mn>1</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=r^{1}+\mathbf{1}[r^{1}=1]*\max_{1&lt;i\leq K}(\mathbf{1}[r^{i}=1]*\mathbf{1}_{skill}(q^{i}|q^{1}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R^{K}" class="ltx_Math" display="inline" id="A2.Ex13.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>K</mi></msup><annotation encoding="application/x-tex">\displaystyle R^{K}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=r^{K}+\mathbf{1}[r^{K}=1]*\mathbf{1}_{skill}(q^{K}|q^{1},...,q^{K-1})" class="ltx_Math" display="inline" id="A2.Ex13.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><msup><mi>r</mi><mi>K</mi></msup><mo>+</mo><mrow><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mi>K</mi></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><msub><mn>ùüè</mn><mrow><mi>s</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>k</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>i</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>q</mi><mi>K</mi></msup><mo fence="false">|</mo><mrow><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mrow><mi>K</mi><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=r^{K}+\mathbf{1}[r^{K}=1]*\mathbf{1}_{skill}(q^{K}|q^{1},...,q^{K-1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.p5">
<p class="ltx_p" id="A2.p5.1">To summarize, the basic idea of Skill-integrated Reward design is that beyond the task completion rewards, the agent model receives additional 1.0 skill relevant rewards in two scenarios:</p>
</div>
<div class="ltx_para" id="A2.p6">
<p class="ltx_p" id="A2.p6.1">Skill Generation Reward - when successfully completed task generates skills that are utilized in at least one subsequent task within the task chain and lead to its success task completion;</p>
</div>
<div class="ltx_para" id="A2.p7">
<p class="ltx_p" id="A2.p7.1">Skill Usage Reward - when the agent effectively applies previously acquired skills from earlier tasks to successfully complete the current task.</p>
</div>
<div class="ltx_para" id="A2.p8">
<p class="ltx_p" id="A2.p8.13">Similar to Section 3.2.4, we describe the general format of SAGE as follows. Given the current policy <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="A2.p8.1.m1" intent=":literal"><semantics><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> and the old policy <math alttext="\pi_{\theta_{old}}" class="ltx_Math" display="inline" id="A2.p8.2.m2" intent=":literal"><semantics><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>d</mi></mrow></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{old}}</annotation></semantics></math>, we first sample task chain with <math alttext="K" class="ltx_Math" display="inline" id="A2.p8.3.m3" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> examples from the original data distribution: <math alttext="(q^{1},q^{2},...,q^{K})\sim Q" class="ltx_Math" display="inline" id="A2.p8.4.m4" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mi>K</mi></msup><mo stretchy="false">)</mo></mrow><mo>‚àº</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">(q^{1},q^{2},...,q^{K})\sim Q</annotation></semantics></math>. For each task pair, we then perform <math alttext="G" class="ltx_Math" display="inline" id="A2.p8.5.m5" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> groups rollout <math alttext="\{\tau_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot|(q^{1},q^{2},...,q^{K}))" class="ltx_math_unparsed" display="inline" id="A2.p8.6.m6" intent=":literal"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>œÑ</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mo>‚àº</mo><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext>old</mtext></msub></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mi>K</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\{\tau_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot|(q^{1},q^{2},...,q^{K}))</annotation></semantics></math>, where <math alttext="\tau_{i}" class="ltx_Math" display="inline" id="A2.p8.7.m7" intent=":literal"><semantics><msub><mi>œÑ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\tau_{i}</annotation></semantics></math> represents the Sequential Rollout trajectory collected by sequentially processing <math alttext="(q^{1},q^{2},...,q^{K})" class="ltx_Math" display="inline" id="A2.p8.8.m8" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mi>K</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q^{1},q^{2},...,q^{K})</annotation></semantics></math>. To differentiate outputs for each task query in <math alttext="\tau_{i}" class="ltx_Math" display="inline" id="A2.p8.9.m9" intent=":literal"><semantics><msub><mi>œÑ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\tau_{i}</annotation></semantics></math>, we define <math alttext="o_{i}^{k}\in\tau_{i}" class="ltx_Math" display="inline" id="A2.p8.10.m10" intent=":literal"><semantics><mrow><msubsup><mi>o</mi><mi>i</mi><mi>k</mi></msubsup><mo>‚àà</mo><msub><mi>œÑ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">o_{i}^{k}\in\tau_{i}</annotation></semantics></math> as the <math alttext="i^{\text{th}}" class="ltx_Math" display="inline" id="A2.p8.11.m11" intent=":literal"><semantics><msup><mi>i</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">i^{\text{th}}</annotation></semantics></math> outputs for <math alttext="q^{k}" class="ltx_Math" display="inline" id="A2.p8.12.m12" intent=":literal"><semantics><msup><mi>q</mi><mi>k</mi></msup><annotation encoding="application/x-tex">q^{k}</annotation></semantics></math> in the group, where <math alttext="k" class="ltx_Math" display="inline" id="A2.p8.13.m13" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> is the chain index. The current policy is then optimized by maximizing the following objective function for skill library integrated GRPO:</p>
</div>
<div class="ltx_para" id="A2.p9">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A12.EGx5">
<tbody id="A2.Ex14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{J}_{Agent}(\theta)" class="ltx_Math" display="inline" id="A2.Ex14.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ùí•</mi><mrow><mi>A</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>g</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>e</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>n</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>t</mi></mrow></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>Œ∏</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{J}_{Agent}(\theta)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathbb{E}_{(q^{1},...,q^{K})\sim Q,\{\tau_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot|(q^{1},...,q^{K}))}" class="ltx_math_unparsed" display="inline" id="A2.Ex14.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><msub><mi>ùîº</mi><mrow><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mi>K</mi></msup><mo stretchy="false">)</mo></mrow><mo>‚àº</mo><mi>Q</mi><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>œÑ</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mo>‚àº</mo><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mtext>old</mtext></msub></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msup><mi>q</mi><mi>K</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></msub></mrow><annotation encoding="application/x-tex">\displaystyle=\mathbb{E}_{(q^{1},...,q^{K})\sim Q,\{\tau_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot|(q^{1},...,q^{K}))}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\frac{1}{G}\sum_{i=1}^{G}\sum_{k=1}^{K}\frac{1}{|o_{i}^{k}|}\sum_{t=1}^{|o_{i}^{k}|}\{\min[s_{i,t}^{k}\hat{A}_{i}^{k}," class="ltx_math_unparsed" display="inline" id="A2.Ex15.m1" intent=":literal"><semantics><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>G</mi></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><msubsup><mi>o</mi><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><msubsup><mi>o</mi><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">|</mo></mrow></munderover></mstyle><mrow><mo stretchy="false">{</mo><mi>min</mi><mrow><mo stretchy="false">[</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi><mi>k</mi></msubsup><mo>,</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\frac{1}{G}\sum_{i=1}^{G}\sum_{k=1}^{K}\frac{1}{|o_{i}^{k}|}\sum_{t=1}^{|o_{i}^{k}|}\{\min[s_{i,t}^{k}\hat{A}_{i}^{k},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex16"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\text{clip}\left(s_{i,t}^{k},1-\epsilon,1+\epsilon\right)\hat{A}_{i}^{k}]\}" class="ltx_math_unparsed" display="inline" id="A2.Ex16.m1" intent=":literal"><semantics><mrow><mrow><mtext>clip</mtext><mrow><mo>(</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><mo>,</mo><mn>1</mn><mo>‚àí</mo><mi>œµ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>œµ</mi><mo>)</mo></mrow><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">]</mo></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\displaystyle\text{clip}\left(s_{i,t}^{k},1-\epsilon,1+\epsilon\right)\hat{A}_{i}^{k}]\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="A2.p10">
<p class="ltx_p" id="A2.p10.8">where <math alttext="s_{i,t}^{k}=\frac{\pi_{\theta}(o_{i,t}^{k}|q^{k},\mathcal{M}_{i}^{k},o_{i,&lt;t}^{k})}{\pi_{\theta_{old}}(o_{i,t}^{k}|q^{k},\mathcal{M}_{i}^{k},o_{i,&lt;t}^{k}))}" class="ltx_math_unparsed" display="inline" id="A2.p10.1.m1" intent=":literal"><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><mo>=</mo><mfrac><mrow><msub><mi>œÄ</mi><mi>Œ∏</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><mo fence="false">|</mo><mrow><msup><mi>q</mi><mi>k</mi></msup><mo>,</mo><msubsup><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><mi>i</mi><mi>k</mi></msubsup><mo>,</mo><msubsup><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow><mi>k</mi></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>œÄ</mi><msub><mi>Œ∏</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>l</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>d</mi></mrow></msub></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>q</mi><mi>k</mi></msup><mo>,</mo><msubsup><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><mi>i</mi><mi>k</mi></msubsup><mo>,</mo><msubsup><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow><mi>k</mi></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">s_{i,t}^{k}=\frac{\pi_{\theta}(o_{i,t}^{k}|q^{k},\mathcal{M}_{i}^{k},o_{i,&lt;t}^{k})}{\pi_{\theta_{old}}(o_{i,t}^{k}|q^{k},\mathcal{M}_{i}^{k},o_{i,&lt;t}^{k}))}</annotation></semantics></math>; and <math alttext="\hat{A}_{i}^{k}=R_{i}^{k}-\text{mean}(\{R_{i}^{k}|i=1,2,..,G\})" class="ltx_math_unparsed" display="inline" id="A2.p10.2.m2" intent=":literal"><semantics><mrow><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi><mi>k</mi></msubsup><mo>=</mo><msubsup><mi>R</mi><mi>i</mi><mi>k</mi></msubsup><mo>‚àí</mo><mtext>mean</mtext><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">{</mo><msubsup><mi>R</mi><mi>i</mi><mi>k</mi></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo lspace="0em" rspace="0.0835em">.</mo><mo lspace="0.0835em" rspace="0.167em">.</mo><mo>,</mo><mi>G</mi><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{A}_{i}^{k}=R_{i}^{k}-\text{mean}(\{R_{i}^{k}|i=1,2,..,G\})</annotation></semantics></math>. <math alttext="s_{i,t}^{k}" class="ltx_Math" display="inline" id="A2.p10.3.m3" intent=":literal"><semantics><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>k</mi></msubsup><annotation encoding="application/x-tex">s_{i,t}^{k}</annotation></semantics></math> represents the importance sampling term; <math alttext="\hat{A}_{i}^{k}" class="ltx_Math" display="inline" id="A2.p10.4.m4" intent=":literal"><semantics><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">\hat{A}_{i}^{k}</annotation></semantics></math> is the advantage computed by the Skill-integrated Reward <math alttext="R_{i}^{k}" class="ltx_Math" display="inline" id="A2.p10.5.m5" intent=":literal"><semantics><msubsup><mi>R</mi><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">R_{i}^{k}</annotation></semantics></math>; <math alttext="\mathcal{M}_{i}^{k}" class="ltx_Math" display="inline" id="A2.p10.6.m6" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">‚Ñ≥</mi><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">\mathcal{M}_{i}^{k}</annotation></semantics></math> denotes the skill library integrated for the query <math alttext="q_{i}^{k}" class="ltx_Math" display="inline" id="A2.p10.7.m7" intent=":literal"><semantics><msubsup><mi>q</mi><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">q_{i}^{k}</annotation></semantics></math> for group <math alttext="i" class="ltx_Math" display="inline" id="A2.p10.8.m8" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A2.p11">
<p class="ltx_p" id="A2.p11.1">To better understand SAGE‚Äôs performance with longer task chains during Sequential Rollout, we extended our experiments by combining all three tasks within one scenario into a continuous task chain. This experiment maintained similar settings to the original one described in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A7" title="Appendix G Training Details of SAGE ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">G</span></a>, with one key modification: both batch size and mini-batch size were increased to 576 to accommodate the additional rollouts required by the longer task chains. Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A2.T6" title="Table 6 ‚Ä£ Appendix B SAGE with Longer Task Chain in Sequential Rollout ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">6</span></a> compares the performance between SAGE using three-example task chain and the original two-example task chain.</p>
</div>
<figure class="ltx_table" id="A2.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance of SAGE with longer task chain.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T6.1" style="width:433.6pt;height:65.2pt;vertical-align:-29.3pt;"><span class="ltx_transformed_inner" style="transform:translate(53.1pt,-8.0pt) scale(1.32459539849352,1.32459539849352) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T6.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T6.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A2.T6.1.1.1.1.1">Task Chain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="A2.T6.1.1.1.1.2">Test Normal</th>
</tr>
<tr class="ltx_tr" id="A2.T6.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="A2.T6.1.1.2.2.1">Length</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A2.T6.1.1.2.2.2">TGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A2.T6.1.1.2.2.3">SGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A2.T6.1.1.2.2.4">Avg. Steps</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A2.T6.1.1.2.2.5">Avg. Tokens</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T6.1.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A2.T6.1.1.3.1.1">2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.1.1.3.1.2"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.3.1.2.1">72.0 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.1.1.3.1.3"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.3.1.3.1">60.7 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.1.1.3.1.4"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.3.1.4.1">12.1 ¬± 0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T6.1.1.3.1.5"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.3.1.5.1">1,475 ¬± 127</span></td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A2.T6.1.1.4.2.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.1.1.4.2.2">70.6 ¬± 3.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.1.1.4.2.3">54.8 ¬± 4.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.1.1.4.2.4">14.3 ¬± 0.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T6.1.1.4.2.5">2,585 ¬± 13</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A2.p12">
<p class="ltx_p" id="A2.p12.1">The results indicate that longer task chains do not necessarily improve the skill library agent‚Äôs performance. This may be attributed to two factors: (1) Reward Distribution Imbalance: Examples in the task chain are typically clustered by similar instructions, leading to most relevant skills being generated during the first example‚Äôs execution. Consequently, the first example predominantly receives rewards for skill generation, while subsequent examples mainly receive rewards for skill utilization, creating an asymmetric reward structure. (2) Gradient Variance: As the chain lengthens, later tasks begin with increasingly divergent skill libraries, deviating from standard GRPO settings. This deviation potentially leads to larger gradient variance, which may adversely affect the final performance. Besides, the iterative nature of Sequential Rollout process results in substantially increased computational costs as the number of tasks grows. Given these challenges and the empirically suboptimal performance, we ultimately chose a two-example task chain structure. This configuration allows the first example to focus on skill generation while the second emphasizes skill usage.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>AppWorld Dataset</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In AppWorld dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">trivedi2024appworld</span>)</cite>, autonomous agents are required to complete everyday digital tasks (e.g., sending messages and transfering money to roommates) by consulting API documentation and executing API calls through generated code. Unlike other API usage datasets, AppWorld provides a high-quality execution environment that simulates 9 everyday apps (Amazon, Spotify, Venmo, Gmail, Todoist, SimpleNote, Splitwise, FileSystem, Phone, and ApiDocs). The environment includes totally 457 APIs and features interactions with over 100 simulated users.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">To benchmark the agent model performance, AppWorld provides a suite of 750 natural, diverse, and challenging tasks, divided into four splits: Train (105), Dev (60), Test-Normal (168), and Test-Challenge (417). The Test-Challenge set not only contains more test examples but also requires APIs from applications (Amazon and Gmail) that are absent from the Train, Dev, and Test-Normal sets. This design specifically evaluates the models‚Äô ability to generalize to unfamiliar APIs.</p>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p" id="A3.p3.1">For accurate evaluation, each task incorporates a manually written program that evaluates the final environment state against predefined criteria, producing a completion rate (0-1) that serves as our outcome-based reward. The stage-based tests accommodate multiple solution paths, enabling more precise reward calculations.
Notably, AppWorld presents a significant challenge even for state-of-the-art LLMs. According to <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">trivedi2024appworld</span></cite>, even GPT-4O achieves only about 49% success rate on Test-Normal tasks and approximately 30% on Test-Challenge tasks.</p>
</div>
<div class="ltx_para" id="A3.p4">
<p class="ltx_p" id="A3.p4.1">Furthermore, the 750 tasks comes from 250 task scenarios, each consisting of three tasks sharing similar instructions under different simulated users. The scenario-based structure of AppWorld makes it well-suited for Sequential Rollout, as tasks within the same scenario naturally form a task chain.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Prompt for Skill Library Agent</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12.F9" title="Figure 9 ‚Ä£ Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">9</span></a> illustrates the prompt template used for our skill library agent. Within the prompt, placeholders (marked in red and enclosed in brackets) are designed to be replaced with corresponding content. The template requires an example task to serve as an in-context example, while the skill library section would be replaced with available function skills. All remaining information is derived directly from the AppWorld dataset‚Äôs task specifications.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Training Details of Baseline GRPO</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">We built our training framework based on the verl repository <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">sheng2024hybridflow</span>)</cite>, which uses vLLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kwon2023efficient</span>)</cite> for rollout and FSDP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhao2023pytorch</span>)</cite> for gradient update. While our training configuration largely followed <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2025reinforcement</span></cite>, there are still some key modifications: we implemented strictly on-policy reinforcement learning and employed full-parameter fine-tuning instead of LoRA. The implementation details of the baseline GRPO used in our paper are shown as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.p2">
<p class="ltx_p" id="A5.p2.1"><span class="ltx_text ltx_font_bold" id="A5.p2.1.1">Training Framework.</span> Our baseline GRPO differs from the original algorithm presented in Section 3.2.1. Here we eliminated the KL penalty and recomputed the advantage of each group as <math alttext="\hat{A}_{i,t}=r_{i}-\text{mean}(\mathbf{r})" class="ltx_Math" display="inline" id="A5.p2.1.m1" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>‚àí</mo><mrow><mtext>mean</mtext><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mi>ùê´</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{A}_{i,t}=r_{i}-\text{mean}(\mathbf{r})</annotation></semantics></math> without normalized by the standard error.</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.p3">
<p class="ltx_p" id="A5.p3.1"><span class="ltx_text ltx_font_bold" id="A5.p3.1.1">Dataset.</span> We only applied the difficulty-1 and difficulty-2 tasks in the Train split of AppWorld dataset for our baseline training.</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.p4">
<p class="ltx_p" id="A5.p4.1"><span class="ltx_text ltx_font_bold" id="A5.p4.1.1">Rollout Settings.</span> For each step of training, we performed the rollout process with 1.0 temperature for 36 random sampled examples with <math alttext="G=8" class="ltx_Math" display="inline" id="A5.p4.1.m1" intent=":literal"><semantics><mrow><mi>G</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">G=8</annotation></semantics></math> agents for each group, resulting in totally 288 rollout. During the agents interact with AppWorld environment, we allowed 40 maximum interaction turns with a limitation of 1,500 output tokens for each turn. Due to the GPU memory limitation, we set the context limitation as 28,048 during rollout. For each task, once its trajectory length tokens exceed the limitation during interaction, the rollout will be stopped and marked as completed. Besides, we observed that the code execution outputs returned by the AppWorld environment may be extremely long (e.g., print all examples in the list). Thus, we applied truncation for returned environment outputs exceeding 12,000 characters and append prompt ‚ÄúObservation truncated for display.‚Äù afterwards as a truncation notification. After each environment output, we also appended a reminder about the task related information before agents generate the next step. Last but not least, to improve the rollout efficiency, we make an early stop for the entire rollout process when at least 25 interaction steps for unfinished rollout, at least 6 rollouts for each task and 90% of the total number of rollouts have been collected.</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.p5">
<p class="ltx_p" id="A5.p5.1"><span class="ltx_text ltx_font_bold" id="A5.p5.1.1">Training Settings.</span> We performed the whole RL training process on 4xNVIDIA H100 8-GPU nodes. Due to the long-horizon trajectories, we utilized the ulysses sequence parallel <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">jacobs2023deepspeed</span>)</cite>, allocating each example on 4 GPUs. We kept the mini batch size the same as the 288 batch size of rollout. A constant learning rate of 1e-6 and clip the gradient norm to 1 were applied for policy gradient update.</p>
</div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Expert Experience Dataset Generation</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">To collect the expert experience dataset for SFT, we implemented a rejection sampling process under our skill library agent with the advanced model Claude Sonnet 3.5 V2 as the expert. Specifically, we conducted rejection sampling on the training set using the expert model with temperatures ranging from 0.05 to 1.0, with increments of 0.05 across 20 steps. The AppWorld dataset comprises 30 distinct scenarios, and we sequentially deployed the skill library agent through all three tasks within each scenario, allowing skills generated in previous tasks to be directly utilized for subsequent tasks within the same scenario. When processing each task, we implemented a maximum retry limit of 10 attempts, terminating sampling within the whole scenario when this limit is reached. Finally, we retained only those scenarios where either all three tasks or at least the first two tasks are successfully completed, as failures in the second task typically indicate potential issues with the skill generation process. This process finally yielded 1,129 valid examples.</p>
</div>
<div class="ltx_para" id="A6.p2">
<p class="ltx_p" id="A6.p2.1">Using these collected examples as an expert experience dataset, we employed Llama-Factory <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zheng2024llamafactory</span>)</cite> to perform full parameter fine-tuning with 4xNVIDIA H100 8-GPU nodes. During fine-tuning, since all trajectories in the expert experience dataset consist of multiple turn interactions, we did gradient update exclusively on the agent responses with prompt and environment outputs masked. The model was trained with a batch size of 128 and employed a learning rate of 1e-6, using a cosine scheduling strategy with 0.1 warmup ratio. Due to Llama-Factory‚Äôs limitations in sequence parallel, we set the maximum token length to 12,500, and any examples exceeding this limit were excluded from the dataset.</p>
</div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Training Details of SAGE</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">To ensure a fair comparison with the baseline GRPO method, we maintained consistency in most training parameters. However, the introduction of the Sequential Rollout process necessitated certain modifications. Below, we detail the specific adjustments made to our training configuration.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p2">
<p class="ltx_p" id="A7.p2.1"><span class="ltx_text ltx_font_bold" id="A7.p2.1.1">Training Framework.</span> Refer to Section 3.2 for SAGE framework used for training.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p3">
<p class="ltx_p" id="A7.p3.1"><span class="ltx_text ltx_font_bold" id="A7.p3.1.1">Dataset.</span> While we continued to utilize difficulty-1 and difficulty-2 tasks from the Train split of AppWorld as our training dataset, we implemented a revised sampling strategy to accommodate the Sequential Rollout process. We adopt a scenario-based sampling approach to construct the required task chains. Specifically, AppWorld tasks are inherently organized into scenarios, with each scenario comprising three distinct tasks. Our sampling process consisted of two steps: we first selected a certain number of task scenarios, and then within each selected scenario, we sampled two tasks to form a task chain.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p4">
<p class="ltx_p" id="A7.p4.1"><span class="ltx_text ltx_font_bold" id="A7.p4.1.1">Rollout Settings.</span> Because the Sequential Rollout process requires iterative interactions within the task chain, the rollout completion time approximately doubles compared to the baseline if we maintain the same total number of rollouts. To accelerate the training process, we required a larger number of total rollouts. For each step of training, we decided to use all scenarios in the Train split of AppWorld with difficulty-1 and difficulty-2 tasks, which only contains 24 scenarios. Sampling two task examples from those scenarios resulted in 48 tasks. Under the same agent number <math alttext="G=8" class="ltx_Math" display="inline" id="A7.p4.1.m1" intent=":literal"><semantics><mrow><mi>G</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">G=8</annotation></semantics></math> for each group, we finally obtained 384 rollouts for each step.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p5">
<p class="ltx_p" id="A7.p5.1"><span class="ltx_text ltx_font_bold" id="A7.p5.1.1">Training Settings.</span> The only change during training was the mini batch size for policy gradient update. Here we aligned it to the batch size of rollout, which is 384.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p6">
<p class="ltx_p" id="A7.p6.1"><span class="ltx_text ltx_font_bold" id="A7.p6.1.1">Training Details.</span> We present the training curve with average training reward per step in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A7.F5" title="Figure 5 ‚Ä£ Appendix G Training Details of SAGE ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">5</span></a>. During training, we saved model checkpoints every 5 steps and evaluated their performance on the Dev set. The checkpoint achieving the highest combined TGC and SGC scores on the Dev set was selected as our final model. Evaluation curve showing the TGC and SGC scores on the Dev set is illustrated in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A7.F6" title="Figure 6 ‚Ä£ Appendix G Training Details of SAGE ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">6</span></a>. The figure clearly demonstrates that the model achieves optimal performance in both TGC and SGC metrics at step 75. Thus, we selected the model checkpoint at step 75 as our final agent model.</p>
</div>
<figure class="ltx_figure" id="A7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="A7.F5.g1" src="train.png" width="238"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Training curve of SAGE.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="A7.F6.g1" src="eval.png" width="238"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>SGC and TGC scores on Dev set for each 5 training steps.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Task Execution Examples with Different Models</h2>
<div class="ltx_para" id="A8.p1">
<p class="ltx_p" id="A8.p1.1">This section presents several real task execution examples on AppWorld, comparing agent models from the baseline GRPO and each stage of our approach. These examples demonstrate the advantages of the skill library agent and the improvements in skill library usage achieved by SAGE. Due to the extensive length of the trajectories, all examples are presented in a summarized format. For each interaction turn, we represent the agent‚Äôs action using an emoji accompanied with a brief description. Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A8.F7" title="Figure 7 ‚Ä£ Appendix H Task Execution Examples with Different Models ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">7</span></a> provides explanations for the meaning of each emoji.</p>
</div>
<figure class="ltx_figure" id="A8.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="44" id="A8.F7.g1" src="emoji.png" width="238"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Action Explanations. Code execution failures encompass various issues including unsuccessful API calls, errors in skill generation, and improper skill usage. Anti-patterns primarily refer to the key actions that ultimately lead to task failures.</figcaption>
</figure>
<div class="ltx_para" id="A8.p2">
<p class="ltx_p" id="A8.p2.1">To clearly demonstrate the skill generation and usage process, we present two task execution examples for each agent model under the same scenario. Examples of the baseline GRPO are shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12.F10" title="Figure 10 ‚Ä£ Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">10</span></a>. The results for the Skill Library Agent, SFT, and SAGE stages of our approach are presented in Figures¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12.F11" title="Figure 11 ‚Ä£ Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12.F12" title="Figure 12 ‚Ä£ Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">12</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12.F13" title="Figure 13 ‚Ä£ Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">13</span></a> respectively.</p>
</div>
<div class="ltx_para" id="A8.p3">
<p class="ltx_p" id="A8.p3.1">Analyzing the task execution examples across different stages of our approach reveals several key observations. The Skill Library Agent, before training, demonstrates poor performance in task execution with the skill library. It often requires multiple attempts to define a executable function, and even then, errors may still occur. Additionally, it tends to simulate data for task processing and exhibits unexpected behaviors such as repetitive generation.</p>
</div>
<div class="ltx_para" id="A8.p4">
<p class="ltx_p" id="A8.p4.1">After SFT, the agent model shows significantly reduced interaction steps and fewer errors in skill generation and usage. However, while these patterns learned from expert experience data minimize basic errors, the SFT model still struggles to achieve successful task completion.</p>
</div>
<div class="ltx_para" id="A8.p5">
<p class="ltx_p" id="A8.p5.1">In comparison to the baseline GRPO, agent model trained by SAGE substantially improves task efficiency by eliminating the need for step-by-step API calls and reducing redundant actions in similar tasks.</p>
</div>
</section>
<section class="ltx_appendix" id="A9">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Retrieval Method Ablation Study</h2>
<div class="ltx_para" id="A9.p1">
<p class="ltx_p" id="A9.p1.1">In this section, we introduce each retrieval method presented in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#S4.T3" title="Table 3 ‚Ä£ 4.4 Ablation Studies ‚Ä£ 4 Experiments ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">3</span></a> and provide more analysis for the results.</p>
</div>
<section class="ltx_subsection" id="A9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">I.1 </span>Details of Retrieval Methods</h3>
<div class="ltx_para ltx_noindent" id="A9.SS1.p1">
<p class="ltx_p" id="A9.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A9.SS1.p1.1.1">Same Scenario.</span> The Same Scenario method serves as our paper‚Äôs default evaluation setting, effectively utilizing the scenario labels provided in the dataset. Under this approach, we construct a task chain comprising three tasks within each scenario, combined with a skill library. The agent performs these tasks sequentially, with skills generated from the first two examples being accumulated in the skill library. This accumulated knowledge is then made available for the subsequent two examples. Importantly, each skill library is constrained to its specific task scenario.</p>
</div>
<div class="ltx_para ltx_noindent" id="A9.SS1.p2">
<p class="ltx_p" id="A9.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A9.SS1.p2.1.1">Query N-gram.</span> The core concept of this retrieval method is to leverage skills generated from tasks with similar queries¬†<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2025survey</span></cite>. Under this approach, each query is associated with its own skill library, which can be directly inherited once being retrieved. For simplicity, we initially implement a model-free retrieval approach based on 2-gram Jaccard Similarity to identify the most similar query. To ensure retrieval quality, we establish a threshold of 0.5 for the Jaccard Similarity score; any retrieved queries falling below this threshold are discarded.</p>
</div>
<div class="ltx_para ltx_noindent" id="A9.SS1.p3">
<p class="ltx_p" id="A9.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="A9.SS1.p3.1.1">Query Embedding.</span> This retrieval method shares the same fundamental principle as Query N-gram, but employs a more advanced text-embedding model to compute embedding similarity for query retrieval. Specifically, we utilize the all-MiniLM-L6-v2 model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">reimers-2019-sentence-bert</span>)</cite> for computing query embeddings with a threshold of 0.65 for the cosine similarity.</p>
</div>
<div class="ltx_para ltx_noindent" id="A9.SS1.p4">
<p class="ltx_p" id="A9.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="A9.SS1.p4.1.1">Skill Embedding.</span> Skill Embedding retrieves relevant skills from the skill library for each task query using a standard retrieval model. Each newly generated skill is encoded into embeddings and indexed in the skill library for future retrieval purposes. We employ Qwen3-Embedding-0.6B <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">qwen3embedding</span>)</cite> as our retrieval model and keep the top 5 retrieved skills for usage. This model differs from the general text-embedding model used for Query Embedding because it is specifically trained for document retrieval, where we treat skills as documents and task instructions as queries.</p>
</div>
</section>
<section class="ltx_subsection" id="A9.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">I.2 </span>Further Analysis</h3>
<div class="ltx_para" id="A9.SS2.p1">
<p class="ltx_p" id="A9.SS2.p1.1">Among the three retrieval methods studied in our ablation study, Query N-gram achieves performance most similar to the ideal Same Scenario case. This is because tasks under the same scenario in AppWorld share the same query structure. Some tasks within the same scenario even use identical query under different simulated users. Queries within the same scenario naturally share high similarity under N-gram metric. With a proper threshold, the skill library can be effectively constrained to skills within the same scenario.</p>
</div>
<div class="ltx_para" id="A9.SS2.p2">
<p class="ltx_p" id="A9.SS2.p2.1">Query Embedding shows slightly lower performance in TGC and SGC but higher efficiency with fewer Avg. Steps and Tokens. Since text embedding models primarily focus on semantic meaning rather than structural patterns, and queries with different structures can convey similar semantic meanings, it is challenging for Query Embedding restricting queries to a single scenario.
On the other hand, with Query Embedding, most initial examples within each task scenario can retrieve similar queries from other scenarios. While this broader skill usage leads to fewer interaction steps and tokens, it doesn‚Äôt necessarily improve accuracy due to difficult cross-scenario skill adaptation.</p>
</div>
<div class="ltx_para" id="A9.SS2.p3">
<p class="ltx_p" id="A9.SS2.p3.1">The Skill Embedding method demonstrates lower performance compared to other approaches. This may be attributed to the inherent difficulty in retrieving useful skill functions based on queries, as it requires understanding tool usage rather than just text relevance. Future work could explore the development of skill/tool-specific retrievers to enhance skill library agent performance.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A10">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix J </span>Reward Design Ablation Study</h2>
<div class="ltx_para" id="A10.p1">
<p class="ltx_p" id="A10.p1.1">This section supplements the reward design ablation study by providing detailed descriptions of alternative reward designs and additional analysis of skill library usage.</p>
</div>
<section class="ltx_subsection" id="A10.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">J.1 </span>Details of Reward Designs</h3>
<div class="ltx_para ltx_noindent" id="A10.SS1.p1">
<p class="ltx_p" id="A10.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A10.SS1.p1.1.1">Outcome-based Reward.</span> Outcome-based Reward design solely relies on the task completion rate as the reward. In this case, skill learning is driven exclusively by Sequential Rollout.</p>
</div>
<div class="ltx_para" id="A10.SS1.p2">
<p class="ltx_p" id="A10.SS1.p2.4">Formally, let <math alttext="r^{1},r^{2}\in[0,1]" class="ltx_Math" display="inline" id="A10.SS1.p2.1.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>,</mo><msup><mi>r</mi><mn>2</mn></msup></mrow><mo>‚àà</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">r^{1},r^{2}\in[0,1]</annotation></semantics></math> denote the verifiable task completion rate of the task chain <math alttext="(q^{1},q^{2})" class="ltx_Math" display="inline" id="A10.SS1.p2.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q^{1},q^{2})</annotation></semantics></math> collected from Sequential Rollout. The Outcome-based Rewards can be directly formulated as <math alttext="R^{1}_{O}=r^{1}" class="ltx_Math" display="inline" id="A10.SS1.p2.3.m3" intent=":literal"><semantics><mrow><msubsup><mi>R</mi><mi>O</mi><mn>1</mn></msubsup><mo>=</mo><msup><mi>r</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">R^{1}_{O}=r^{1}</annotation></semantics></math> and <math alttext="R^{2}_{O}=r^{2}" class="ltx_Math" display="inline" id="A10.SS1.p2.4.m4" intent=":literal"><semantics><mrow><msubsup><mi>R</mi><mi>O</mi><mn>2</mn></msubsup><mo>=</mo><msup><mi>r</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^{2}_{O}=r^{2}</annotation></semantics></math>. Similar to the Skill-integrated Reward design, we impose a -1.0 penalty reward specifically on responses where the agent provides no code and terminates the task.</p>
</div>
<div class="ltx_para ltx_noindent" id="A10.SS1.p3">
<p class="ltx_p" id="A10.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="A10.SS1.p3.1.1">Chain-based Reward.</span> Chain-based Reward design adds a bonus reward of 1.0 when all tasks within the task chain are successfully completed, rather than rewarding the skill generation and usage throughout the execution. This design aims to test the necessity of tracking precise skill usage during the rollout process.</p>
</div>
<div class="ltx_para ltx_noindent" id="A10.SS1.p4">
<p class="ltx_p" id="A10.SS1.p4.4">Formally, let <math alttext="r^{1},r^{2}\in[0,1]" class="ltx_Math" display="inline" id="A10.SS1.p4.1.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>,</mo><msup><mi>r</mi><mn>2</mn></msup></mrow><mo>‚àà</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">r^{1},r^{2}\in[0,1]</annotation></semantics></math> denote the verifiable outcome-based rewards of the task chain <math alttext="(q^{1},q^{2})" class="ltx_Math" display="inline" id="A10.SS1.p4.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>q</mi><mn>1</mn></msup><mo>,</mo><msup><mi>q</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q^{1},q^{2})</annotation></semantics></math> collected from Sequential Rollout. We can formulate the Chain-based Rewards <math alttext="R^{1}_{C}" class="ltx_Math" display="inline" id="A10.SS1.p4.3.m3" intent=":literal"><semantics><msubsup><mi>R</mi><mi>C</mi><mn>1</mn></msubsup><annotation encoding="application/x-tex">R^{1}_{C}</annotation></semantics></math> and <math alttext="R^{2}_{C}" class="ltx_Math" display="inline" id="A10.SS1.p4.4.m4" intent=":literal"><semantics><msubsup><mi>R</mi><mi>C</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">R^{2}_{C}</annotation></semantics></math> as:</p>
<table class="ltx_equation ltx_eqn_table" id="A10.Ex17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{cases}R^{1}_{C}=r^{1}+\mathbf{1}[r^{1}=1]*\mathbf{1}[r^{2}=1]\\
R^{2}_{C}=r^{2}+\mathbf{1}[r^{1}=1]*\mathbf{1}[r^{2}=1]\end{cases}" class="ltx_Math" display="block" id="A10.Ex17.m1" intent=":literal"><semantics><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><msubsup><mi>R</mi><mi>C</mi><mn>1</mn></msubsup><mo>=</mo><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>+</mo><mrow><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><mn>ùüè</mn></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></mtd><mtd></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><msubsup><mi>R</mi><mi>C</mi><mn>2</mn></msubsup><mo>=</mo><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>+</mo><mrow><mrow><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mn>1</mn></msup><mo>=</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">‚àó</mo><mn>ùüè</mn></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></mtd><mtd></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{cases}R^{1}_{C}=r^{1}+\mathbf{1}[r^{1}=1]*\mathbf{1}[r^{2}=1]\\
R^{2}_{C}=r^{2}+\mathbf{1}[r^{1}=1]*\mathbf{1}[r^{2}=1]\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A10.SS1.p4.5">where indicator <math alttext="\mathbf{1}[r=1]" class="ltx_Math" display="inline" id="A10.SS1.p4.5.m1" intent=":literal"><semantics><mrow><mn>ùüè</mn><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">[</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{1}[r=1]</annotation></semantics></math> represents whether the outcome-based reward equals 1.
Similar to the Skill-integrated Reward design, we impose a -1.0 penalty reward specifically on responses where the agent provides no code and terminates the task.</p>
</div>
</section>
<section class="ltx_subsection" id="A10.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">J.2 </span>Skill Library Usage Analysis</h3>
<figure class="ltx_figure" id="A10.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="119" id="A10.F8.g1" src="model_comparison_chart_ablation.png" width="238"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Analysis of skill usage patterns across different reward designs. Performance metrics are shown as ratios relative to Skill-integrated Reward design, with numerical values annotated.</figcaption>
</figure>
<div class="ltx_para" id="A10.SS2.p1">
<p class="ltx_p" id="A10.SS2.p1.1">To better illustrate the advantages of Skill-integrated Reward design, we conducted a detailed skill library usage analysis similar to Section 4.3 for agent models trained with different reward designs. Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A10.F8" title="Figure 8 ‚Ä£ J.2 Skill Library Usage Analysis ‚Ä£ Appendix J Reward Design Ablation Study ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">8</span></a> summarizes the relative performance changes when comparing alternative reward designs to our Skill-integrated Reward.
The results reveal that while the agent model trained with Skill-integrated Reward exhibits significantly more skill usage behaviors (Skill Usage Rate, Skill Library Size and Used Skill Num), all three reward designs maintain similar Success Skill Usage Rate. This indicates that our reward design substantially enhances skill usage frequency, but successful skill application appears to be a general behavior achievable across different reward designs.
Furthermore, the smaller Skill Library sizes and fewer Used Skill Nums in both Outcome-based and Chain-based Rewards explain their higher Avg. Steps but lower Avg. Tokens compared to Skill-integrated Reward.
This pattern emerges because while skill usage typically reduces interaction steps, the generation of skill functions generally requires more tokens. Interestingly, the Chain-based Reward shows notably lower skill usage behaviors compared to even the Outcome-based Reward. This phenomenon might be attributed to the early training stages where, despite successful task completion in the task chain, skill usage rates remain low. Consequently, the additional reward added in Chain-based Reward may inadvertently reinforce behavior patterns that exclude skill usage.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A11">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix K </span>RL Initialization Methods</h2>
<div class="ltx_para" id="A11.p1">
<p class="ltx_p" id="A11.p1.1">This section details various initialization methods before implementing SAGE.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p2">
<p class="ltx_p" id="A11.p2.1"><span class="ltx_text ltx_font_bold" id="A11.p2.1.1">Base Model.</span> In this approach, we directly apply the base model, Qwen2.5-32B-Instruct, to the RL process without any additional initialization.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p3">
<p class="ltx_p" id="A11.p3.1"><span class="ltx_text ltx_font_bold" id="A11.p3.1.1">Self Distillation.</span> Like SFT initialization, Self Distillation employs supervised fine-tuning. However, instead of using Claude to generate the expert experience dataset, it utilizes the base model (Qwen2.5-32B-Instruct) for data generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p4">
<p class="ltx_p" id="A11.p4.1"><span class="ltx_text ltx_font_bold" id="A11.p4.1.1">RL Warm-Up.</span> This method initializes SAGE through a preliminary RL process. It employs baseline GRPO with the skill library agent prompt, but without actual skill library involvement, to familiarize the model with the agent format before implementing SAGE.</p>
</div>
</section>
<section class="ltx_appendix" id="A12">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix L </span>SFT Initialized Baseline GRPO</h2>
<div class="ltx_para" id="A12.p1">
<p class="ltx_p" id="A12.p1.1">For the SFT initialized baseline GRPO, we used the same SFT model checkpoint as in SAGE. Unlike the original baseline GRPO settings described in Appendix¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A5" title="Appendix E Training Details of Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">E</span></a>, this SFT-initialized version utilizes the skill library agent framework without engaging the skill library, rather than using the original ReAct framework. Specifically, we employed the skill library agent prompt illustrated in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12.F9" title="Figure 9 ‚Ä£ Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">9</span></a> to guide the rollout process, wherein the model first generates a function and then executes it to process the task. Since the baseline GRPO operates without the skill library, we applied an empty skill library to the prompt placeholder for each task during rollout. The comparative results among this SFT initialized baseline GRPO, the original baseline GRPO, and SAGE are presented in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12.T7" title="Table 7 ‚Ä£ Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">7</span></a>. To align with the training process, only SAGE in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2512.17102v1#A12.T7" title="Table 7 ‚Ä£ Appendix L SFT Initialized Baseline GRPO ‚Ä£ Reinforcement Learning for Self-Improving Agent with Skill Library"><span class="ltx_text ltx_ref_tag">7</span></a> performs the evaluation with skill library involved.</p>
</div>
<div class="ltx_para" id="A12.p2">
<p class="ltx_p" id="A12.p2.1">From the table, we observe that SFT may not benefit the baseline training, resulting in even worse performance than the original baseline. On the other hand, SFT initialized baseline GRPO demonstrates lower average generated tokens. This phenomenon may be attributed to the agent model remaining constrained by the initial patterns acquired through SFT.</p>
</div>
<figure class="ltx_table" id="A12.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Performance of SFT Initialized Baseline GRPO.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A12.T7.1" style="width:433.6pt;height:71.4pt;vertical-align:-32.8pt;"><span class="ltx_transformed_inner" style="transform:translate(30.8pt,-5.1pt) scale(1.16560269201174,1.16560269201174) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A12.T7.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A12.T7.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A12.T7.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="A12.T7.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="A12.T7.1.1.1.1.2">Test Normal</th>
</tr>
<tr class="ltx_tr" id="A12.T7.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A12.T7.1.1.2.2.1">TGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A12.T7.1.1.2.2.2">SGC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A12.T7.1.1.2.2.3">Avg. Steps</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A12.T7.1.1.2.2.4">Avg. Tokens</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A12.T7.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A12.T7.1.1.3.1.1">GRPO</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A12.T7.1.1.3.1.2">69.2 ¬± 2.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A12.T7.1.1.3.1.3">51.8 ¬± 5.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A12.T7.1.1.3.1.4">16.4 ¬± 0.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A12.T7.1.1.3.1.5">3,613 ¬± 200</td>
</tr>
<tr class="ltx_tr" id="A12.T7.1.1.4.2">
<td class="ltx_td ltx_align_center" id="A12.T7.1.1.4.2.1">SFT Initialized GRPO</td>
<td class="ltx_td ltx_align_center" id="A12.T7.1.1.4.2.2">66.1 ¬± 1.3</td>
<td class="ltx_td ltx_align_center" id="A12.T7.1.1.4.2.3">51.2 ¬± 0.8</td>
<td class="ltx_td ltx_align_center" id="A12.T7.1.1.4.2.4">12.8 ¬± 0.1</td>
<td class="ltx_td ltx_align_center" id="A12.T7.1.1.4.2.5"><span class="ltx_text ltx_font_bold" id="A12.T7.1.1.4.2.5.1">1,284 ¬± 18</span></td>
</tr>
<tr class="ltx_tr" id="A12.T7.1.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A12.T7.1.1.5.3.1">SAGE</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A12.T7.1.1.5.3.2"><span class="ltx_text ltx_font_bold" id="A12.T7.1.1.5.3.2.1">72.0 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A12.T7.1.1.5.3.3"><span class="ltx_text ltx_font_bold" id="A12.T7.1.1.5.3.3.1">60.7 ¬± 1.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A12.T7.1.1.5.3.4"><span class="ltx_text ltx_font_bold" id="A12.T7.1.1.5.3.4.1">12.1 ¬± 0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A12.T7.1.1.5.3.5">1,475 ¬± 127</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="A12.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="623" id="A12.F9.g1" src="prompt.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Prompt for Skill Library Agent.</figcaption>
</figure>
<figure class="ltx_figure" id="A12.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="254" id="A12.F10.g1" src="example_grpo.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Task Execution Examples for Baseline GRPO</figcaption>
</figure>
<figure class="ltx_figure" id="A12.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="A12.F11.g1" src="example_slagent.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Task Execution Examples for Skill Library Agent</figcaption>
</figure>
<figure class="ltx_figure" id="A12.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="137" id="A12.F12.g1" src="example_sft.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Task Execution Examples for SFT</figcaption>
</figure>
<figure class="ltx_figure" id="A12.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="198" id="A12.F13.g1" src="example_sage_75.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Task Execution Examples for SAGE</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Dec 18 21:50:15 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
